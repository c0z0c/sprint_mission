---
layout: default
title: "전이학습(Transfer Learning)"
description: "전이학습의 개념, 플로우, 그리고 실제 구현 예제"
date: 2025-08-14
cache-control: no-cache
expires: 0
pragma: no-cache
author: "김명환"
---

# 전이학습(Transfer Learning)

## 1. 전이학습이란?

전이학습(Transfer Learning)은 기존에 학습된 모델의 지식을 새로운 작업에 활용하는 딥러닝 기법입니다. 이미 대용량 데이터셋으로 훈련된 모델의 가중치를 활용하여, 적은 데이터로도 높은 성능을 달성할 수 있습니다.

### 전이학습의 핵심 아이디어
- **사전 훈련된 모델**: ImageNet 같은 대용량 데이터셋으로 이미 훈련된 모델 활용
- **특징 재사용**: 하위 레이어는 범용적인 특징을 학습하므로 재사용 가능
- **효율성**: 처음부터 훈련하는 것보다 시간과 컴퓨팅 자원을 크게 절약

## 2. 전이학습 플로우

```
[사전 훈련된 모델] → [특징 추출기] → [새로운 분류기] → [파인튜닝] → [완성된 모델]
```

### 단계별 설명

#### 단계 1: 사전 훈련된 모델 로드
```python
# 예: ResNet18 사전 훈련된 모델 로드
model = torchvision.models.resnet18(pretrained=True)
```

#### 단계 2: 분류기 교체
```python
# 마지막 fully connected layer를 새로운 클래스 수에 맞게 교체
model.fc = nn.Linear(model.fc.in_features, num_classes)
```

#### 단계 3: 레이어 동결 (선택적)
```python
# 특징 추출기 부분을 동결하여 가중치 업데이트 방지
for param in model.parameters():
    param.requires_grad = False
model.fc.requires_grad = True  # 분류기만 학습
```

#### 단계 4: 훈련 및 파인튜닝
```python
# 낮은 학습률로 전체 모델 파인튜닝
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

## 3. 전이학습 전략

### Feature Extraction (특징 추출)
- 사전 훈련된 모델의 가중치를 고정
- 새로운 분류기만 훈련
- 데이터가 적고 원본 데이터셋과 유사할 때 사용

### Fine-tuning (파인튜닝)
- 사전 훈련된 모델의 가중치를 조금씩 조정
- 전체 네트워크를 낮은 학습률로 훈련
- 충분한 데이터가 있을 때 사용

## 4. 실제 구현 예제

### MNIST를 이용한 간단한 전이학습 예제

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision import datasets

# 1. 데이터 준비
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # ResNet 입력 크기로 조정
    transforms.Grayscale(num_output_channels=3),  # RGB로 변환
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225])  # ImageNet 정규화
])

# MNIST 데이터셋 로드
train_dataset = datasets.MNIST(root='./data', train=True, 
                              download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, 
                             download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# 2. 사전 훈련된 모델 로드 및 수정
model = torchvision.models.resnet18(pretrained=True)

# 마지막 레이어를 MNIST 클래스 수(10)에 맞게 수정
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)

# 3. 손실함수와 옵티마이저 설정
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. 훈련 함수
def train_model(model, train_loader, criterion, optimizer, epochs=5):
    # 모델이 위치한 디바이스로 데이터를 옮기기 위해 디바이스 결정
    device = next(model.parameters()).device
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, '
                      f'Loss: {loss.item():.4f}, '
                      f'Acc: {100.*correct/total:.2f}%')
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        print(f'Epoch {epoch+1} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%')

# 5. 평가 함수
def evaluate_model(model, test_loader):
    # 평가도 모델이 있는 디바이스에서 수행
    device = next(model.parameters()).device
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            _, predicted = torch.max(output, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    accuracy = 100. * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')
    return accuracy

# 6. 모델 훈련
print("=== 전이학습 시작 ===")
train_model(model, train_loader, criterion, optimizer, epochs=3)

# 7. 모델 평가
print("\n=== 모델 평가 ===")
test_accuracy = evaluate_model(model, test_loader)

# 8. 모델 저장
torch.save(model.state_dict(), 'mnist_transfer_model.pth')
print("모델이 저장되었습니다: mnist_transfer_model.pth")
```

### load_state_dict를 사용한 모델 로딩

```python
# 저장된 모델 로딩 예제
def load_trained_model(model_path, num_classes=10):
    # 모델 구조 재생성
    model = torchvision.models.resnet18(pretrained=False)
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    
    # 저장된 가중치 로드
    # map_location을 사용해 CPU/GPU 호환 로딩
    state = torch.load(model_path, map_location=torch.device('cpu'))
    model.load_state_dict(state)
    model.eval()  # 평가 모드로 설정
    
    return model

# 사용 예시
loaded_model = load_trained_model('mnist_transfer_model.pth')
print("모델이 성공적으로 로드되었습니다!")

# 로드된 모델로 예측
def predict_sample(model, test_loader):
    model.eval()
    data_iter = iter(test_loader)
    images, labels = next(data_iter)
    
    # 모델과 동일한 디바이스로 입력 이동
    device = next(model.parameters()).device
    images = images.to(device)

    with torch.no_grad():
        outputs = model(images[:5])  # 첫 5개 샘플만 예측
        _, predicted = torch.max(outputs, 1)
    
    print("실제 레이블:", labels[:5].numpy())
    print("예측 결과:", predicted.cpu().numpy())

predict_sample(loaded_model, test_loader)
```

## 5. 전이학습의 장단점

### 장점
- **빠른 수렴**: 사전 훈련된 가중치로 시작하여 빠르게 학습
- **적은 데이터**: 소규모 데이터셋으로도 좋은 성능 달성
- **계산 효율성**: 처음부터 훈련하는 것보다 시간과 자원 절약
- **높은 성능**: 일반적으로 scratch보다 더 좋은 결과

### 단점
- **도메인 차이**: 원본 데이터와 너무 다르면 효과 제한
- **메모리 사용**: 큰 사전 훈련된 모델은 많은 메모리 필요
- **제약성**: 사전 훈련된 모델의 구조에 의존

## 6. 실무 팁

### 학습률 설정
```python
# 특징 추출기는 낮은 학습률, 분류기는 높은 학습률
optimizer = optim.Adam([
    {'params': model.features.parameters(), 'lr': 1e-4},
    {'params': model.classifier.parameters(), 'lr': 1e-3}
])
```

### 점진적 언프리징
```python
# 처음에는 분류기만 훈련
for param in model.features.parameters():
    param.requires_grad = False

# 나중에 전체 모델 파인튜닝
for param in model.parameters():
    param.requires_grad = True
```

## 7. 결론

전이학습은 현대 딥러닝에서 필수적인 기법입니다. 특히 컴퓨터 비전 분야에서 ImageNet으로 사전 훈련된 모델들은 다양한 작업에서 탁월한 성능을 보여줍니다. 적절한 전략 선택과 하이퍼파라미터 튜닝을 통해 효과적으로 활용할 수 있습니다.

### 다음 단계
- 다양한 사전 훈련된 모델 실험 (VGG, ResNet, EfficientNet 등)
- 데이터 증강(Data Augmentation) 기법 적용
- 앙상블 학습과 전이학습 결합
- 도메인 적응(Domain Adaptation) 기법 탐구