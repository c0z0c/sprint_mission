---
layout: default
title: "스프린트미션6 4팀_김명환 - SSD를 활용한 Pet Face Detection 미션 보고서"
description: "SSD를 활용한 Pet Face Detection 미션 보고서"
cache-control: no-cache
date: 2025-08-23
expires: 0
pragma: no-cache
---

# SSD를 활용한 Pet Face Detection 미션 보고서

## 목차

1. [프로젝트 개요](#1-프로젝트-개요)<br/>
2. [SSD 모델 아키텍처 분석](#2-ssd-모델-아키텍처-분석)<br/>
   - 2.1. [SSD 기본 원리](#21-ssd-기본-원리)<br/>
   - 2.2. [Multi-Scale Feature Maps](#22-multi-scale-feature-maps)<br/>
   - 2.3. [Default Box Generation](#23-default-box-generation)<br/>
3. [모델 구현 및 비교](#3-모델-구현-및-비교)<br/>
   - 3.1. [SSD300_VGG16 (전이학습 방식)](#31-ssd300_vgg16-전이학습-방식)<br/>
   - 3.2. [SSD300_ResNet34 (커스텀 백본)](#32-ssd300_resnet34-커스텀-백본)<br/>
   - 3.3. [아키텍처 비교 분석](#33-아키텍처-비교-분석)<br/>
4. [데이터셋 및 전처리](#4-데이터셋-및-전처리)<br/>
5. [학습 전략 및 하이퍼파라미터](#5-학습-전략-및-하이퍼파라미터)<br/>
6. [성능 평가 결과](#6-성능-평가-결과)<br/>
7. [결론 및 분석](#7-결론-및-분석)<br/>
8. [용어 목록](#8-용어-목록)<br/>

---

## 1. 프로젝트 개요

### 1.1. 미션 목표
Oxford-IIIT Pet Dataset을 활용하여 개와 고양이의 얼굴 영역을 감지하는 Object Detection 모델을 구현합니다. Single Shot MultiBox Detector (SSD) 아키텍처를 기반으로 두 가지 백본 네트워크를 비교 분석하였습니다.

### 1.2. 주요 과제
- **데이터셋**: Oxford-IIIT Pet Dataset (37종의 개와 고양이)
- **모델**: SSD300 아키텍처 (VGG16 vs ResNet34 백본)
- **클래스**: 3개 클래스 (background, cat, dog)
- **평가 지표**: mAP (Mean Average Precision), IoU (Intersection over Union)


<img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/1_2_mermaid_%EC%A3%BC%EC%9A%94%EA%B3%BC%EC%A0%9C.png"/>

--

### 1.3 OXFORD-IIIT PET Dataset (옥스포드-IIIT 애완동물 데이터셋)
*Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman and C. V. Jawahar*

우리는 각 클래스별로 약 200개의 이미지로 구성된 **37개 범주의 애완동물 데이터셋**을 만들었습니다. 이 이미지들은 크기, 자세, 조명에서 큰 변화를 보입니다. 모든 이미지에는 품종, 머리 ROI(관심 영역), 그리고 픽셀 단위의 트라이맵(trimap) 분할에 대한 진실값(ground truth) 주석이 포함되어 있습니다.

---

### 1.4 **내용물**

* **`trimaps/`**
    * 데이터셋의 모든 이미지에 대한 트라이맵 주석
    * **픽셀 주석**: 1: 전경(Foreground), 2: 배경(Background), 3: 분류되지 않음(Not classified)
* **`xmls/`**
    * PASCAL VOC 형식의 머리 경계 상자(bounding box) 주석
* **`list.txt`**
    * 데이터셋의 모든 이미지를 결합한 목록 파일
    * **형식**: `Image CLASS-ID SPECIES BREED ID`
        * **ID**: 1~37번 클래스 ID
        * **SPECIES(종)**: 1: 고양이, 2: 개
        * **BREED ID(품종 ID)**: 고양이는 1~25, 개는 1~12
    * 첫 글자가 **대문자**인 이미지는 고양이, **소문자**인 이미지는 개입니다.
* **`trainval.txt`** / **`test.txt`**
    * 논문에서 사용된 분할(split)을 설명하는 파일입니다. 하지만 무작위 분할을 시도해 보는 것을 권장합니다.

- 이미지 셈플

| Train | Test |
|-------|------|
| <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/train0.png" width="320px"/> | <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/test0.png" width="180px"/> |

- 종 분포

| Train | Test |
|-------|------|
| <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/%EC%9B%90%EB%B3%B8_Train_%EA%B3%A0%EC%96%91%EC%9D%B4_%EA%B0%9C_%EC%A2%85%EB%B6%84%ED%8F%AC.png" width="320px"/> | <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/%EC%9B%90%EB%B3%B8_Test_%EA%B3%A0%EC%96%91%EC%9D%B4_%EA%B0%9C_%EC%A2%85%EB%B6%84%ED%8F%AC.png" width="320px"/> |

- 품종 분포

| Train | Test |
|-------|------|
| <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/%EC%9B%90%EB%B3%B8_Train_%EA%B3%A0%EC%96%91%EC%9D%B4_%ED%92%88%EC%A2%85_%EB%B6%84%ED%8F%AC.png" width="320px"/> | <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/%EC%9B%90%EB%B3%B8_Train_%EA%B0%95%EC%95%84%EC%A7%80_%ED%92%88%EC%A2%85_%EB%B6%84%ED%8F%AC.png" width="320px"/> |

---

### 1.5 **지원 및 참고 문헌**

* **지원**: 문의사항은 Omkar Parkhi (omkar@robots.ox.ac.uk)에게 연락하세요.
* **참고 문헌**:
    * [1] O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar, "Cats and Dogs,"*IEEE Conference on Computer Vision and Pattern Recognition*, 2012

---

## 2. SSD 모델 아키텍처 분석

### 2.1. SSD 기본 원리

SSD (Single Shot MultiBox Detector)는 한 번의 forward pass로 객체의 위치와 클래스를 동시에 예측하는 원샷 디텍션(one-shot detection) 모델입니다. YOLO와 달리 다양한 스케일의 특징맵을 활용하여 크기가 다른 객체들을 효과적으로 감지할 수 있습니다.

#### 핵심 특징:
- **속도**: 실시간 처리 가능한 빠른 속도
- **정확도**: 다중 스케일 특징맵으로 높은 정확도
- **유연성**: 다양한 백본 네트워크 활용 가능

### 2.2. Multi-Scale Feature Maps

SSD의 핵심은 서로 다른 해상도의 특징맵에서 객체를 감지하는 것입니다. 각 특징맵은 서로 다른 크기의 객체를 담당합니다.

$$\text{Feature Map Size} = \frac{\text{Input Size}}{\text{Stride}}$$

<img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/2_2_mermaid_Multi-Scale_Feature_Maps.png"/>

### 2.3. Default Box Generation

각 특징맵의 픽셀 위치에서 다양한 크기와 비율의 앵커 박스(default box)를 생성합니다.

$$\text{Box Width} = s_k \sqrt{a_r}$$
$$\text{Box Height} = \frac{s_k}{\sqrt{a_r}}$$

여기서 $s_k$는 스케일, $a_r$은 어스펙트 레이셔(aspect ratio)입니다.

---

## 3. 모델 구현 및 비교

### 3.1. SSD300_VGG16 (전이학습 방식)

#### 3.1.1. 구현 전략
사전훈련된 COCO 데이터셋의 SSD300_VGG16 모델을 Pet Detection 태스크에 맞게 전이학습합니다.

**핵심 수정사항:**
- **Classification Head 교체**: 91개 클래스 → 3개 클래스
- **Backbone 유지**: VGG16 특징추출기 그대로 활용
- **Regression Head 유지**: 바운딩 박스 예측 부분 재사용

#### 3.1.2. 아키텍처 분석

**백본 구조 (VGG16 기반):**
```
features: Conv2d layers (0-22) - 기본 VGG16 구조
extra: ModuleList (0-4) - 추가 특징맵 생성
├── MaxPool + Conv layers
├── Dilated Convolution (dilation=6)
└── Progressive downsampling
```

**헤드 구조 변화:**
```
Before (COCO): [364, 546, 546, 546, 364, 364] channels
After (Pet):   [12, 18, 18, 18, 12, 12] channels
```

채널 수 계산: `num_anchors × num_classes`
- 앵커 수 4: 4 × 3 = 12 채널
- 앵커 수 6: 6 × 3 = 18 채널

### 3.2. SSD300_ResNet34 (커스텀 백본)

#### 3.2.1. 커스텀 백본 설계

ResNet34를 기반으로 한 완전히 새로운 SSD 백본을 구현했습니다.

```python
class SSDBackboneResnet34(nn.Module):
    def __init__(self, backbone):
        # ResNet34의 layer0-layer3를 features로 활용
        # layer4를 포함한 6개의 extra layers 구성
        # 7개 레벨의 특징맵 생성 (vs VGG16의 6개)
```

**특징맵 생성 과정:**
1. **features**: ResNet34의 layer0~layer3 연결
2. **upsampling**: 256→512 채널 변환
3. **extra**: 6개의 sequential 블록으로 점진적 다운샘플링

#### 3.2.2. 앵커 설정 차이

**VGG16 vs ResNet34 비교:**
```
VGG16:    6개 특징맵, aspect_ratios=[[2], [2,3], [2,3], [2,3], [2], [2]]
ResNet34: 7개 특징맵, aspect_ratios=[[2], [2,3], [2,3], [2,3], [2,3], [2], [2]]
```

### 3.3. 아키텍처 비교 분석

<img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/3_3_%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98_%EB%B9%84%EA%B5%90_%EB%B6%84%EC%84%9D1.png"/>

<img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/3_3_%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98_%EB%B9%84%EA%B5%90_%EB%B6%84%EC%84%9D2.png"/>

**주요 차이점:**

| 구분 | VGG16 | ResNet34 |
|------|-------|----------|
| 백본 타입 | 전이학습 (COCO) | 커스텀 구현 |
| 특징맵 수 | 6개 | 7개 |
| 스케일 수 | 7개 | 8개 |
| 구현 복잡도 | 낮음 | 높음 |
| 유연성 | 제한적 | 높음 |

---

## 4. 데이터셋 및 전처리

### 4.1. Oxford-IIIT Pet Dataset

**데이터셋 구성:**
- **이미지**: 37종의 개와 고양이 품종
- **어노테이션**: XML 형식의 바운딩 박스 정보
- **클래스**: background, cat, dog (3개 클래스)

### 4.2. 전처리 파이프라인

**GeneralizedRCNNTransform 적용:**
```
정규화: mean=[0.48235, 0.45882, 0.40784], std=[0.00392, 0.00392, 0.00392]
리사이징: min_size=(300,), max_size=300, mode='bilinear'
```

**transform ImageNet 적용:**
```
정규화: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
리사이징: min_size=(300,), max_size=300, mode='bilinear'
```

---

## 5. 학습 전략 및 하이퍼파라미터

### 5.1. 옵티마이저 설정

**공통 하이퍼파라미터:**
```python
optimizer = torch.optim.SGD(
    model.parameters(), 
    lr=0.001, 
    momentum=0.9, 
    weight_decay=0.0005
)

scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer, 
    step_size=3, 
    gamma=0.1
)
```

### 5.2. 학습 전략

- **에폭 수**: 15 (전이학습 적합)
- **배치 크기**: 시스템 메모리에 따라 조정
- **평가 함수**: `evaluate_mAP` 사용
- **조기 종료**: 최고 성능 모델 저장

---

## 6. 성능 평가 결과

### 6.1. mAP (Mean Average Precision) 결과

| 모델 | Best Epoch | Best mAP | Best Loss | 학습 파일 |
|------|------------|----------|-----------|-----------|
| **SSD300_ResNet34** | 10 | **0.9979** | 1.5874 | ssd300_resnet34_20250831_184247.pth |
| **SSD300_VGG16** | 4 | **0.9895** | 1.4673 | ssd300_vgg16_20250831_182806.pth |

- 모델별 학습 그래프

| SSD300_VGG16 | SSD300_ResNet34 |
|-------|------|
| <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/SSD300_VGG16_%ED%95%99%EC%8A%B5.png" width="320px"/> | <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/SSD300_ResNet34_%ED%95%99%EC%8A%B5.png" width="320px"/> |

- SSD300_VGG16

| 예측 | 정답 |
|-------|------|
| <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/SSD300_VGG16_Train_%EC%98%88%EC%B8%A1%EB%B0%95%EC%8A%A4.png" width="320px"/> | <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/SSD300_VGG16_Train_%EC%A0%95%EB%8B%B5%EB%B0%95%EC%8A%A4.png" width="320px"/> |


- SSD300_ResNet34

| 예측 | 정답 |
|-------|------|
| <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/SSD300_ResNet34_Train_%EC%98%88%EC%B8%A1%EB%B0%95%EC%8A%A4.png" width="320px"/> | <img src="https://c0z0c.github.io/sprint_mission/%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8%EB%AF%B8%EC%85%98_%EC%99%84%EB%A3%8C/image/07_4%ED%8C%80_%EA%B9%80%EB%AA%85%ED%99%98/SSD300_ResNet34_Train_%EC%A0%95%EB%8B%B5%EB%B0%95%EC%8A%A4.png" width="320px"/> |


### 6.2. 성능 분석

#### 6.2.1. ResNet34 우수성 요인
1. **더 많은 특징맵**: 7개 vs 6개로 더 세밀한 객체 감지
2. **ResNet 아키텍처**: Skip connection으로 더 나은 특징 학습
3. **유연한 앵커 설정**: 다양한 크기의 객체에 대한 적응성

#### 6.2.2. 수렴 속도 비교
- **VGG16**: 4 에폭에서 조기 수렴 (0.9895 mAP)
- **ResNet34**: 10 에폭에서 최고 성능 도달 (0.9979 mAP)

전이학습의 VGG16이 더 빠르게 수렴하지만, 커스텀 ResNet34가 더 높은 최종 성능을 보였습니다.

### 6.3. 손실 함수 분석

SSD의 손실 함수는 분류 손실과 위치 손실의 가중합입니다:

$$L = \frac{1}{N}[L_{conf}(x,c) + \alpha L_{loc}(x,l,g)]$$

여기서:
- $L_{conf}$: 분류 손실 (Cross Entropy)
- $L_{loc}$: 위치 손실 (Smooth L1 Loss)
- $\alpha$: 위치 손실 가중치 (일반적으로 1)
- $N$: 매칭된 기본 박스 수

---

## 7. 결론 및 분석

### 7.1. 주요 성과

1. **높은 성능**: 두 모델 모두 0.98 이상의 mAP 달성
2. **모델 비교**: ResNet34 백본이 VGG16 대비 0.84%p 높은 성능
3. **전이학습 효과**: 사전훈련된 모델을 통한 빠른 수렴 확인

### 7.2. 기술적 인사이트

#### 7.2.1. 전이학습 vs 커스텀 구현
- **전이학습**: 빠른 개발, 안정적 성능, 제한된 커스터마이징
- **커스텀 구현**: 높은 성능, 완전한 제어, 개발 복잡도 증가

#### 7.2.2. 아키텍처 설계 원칙
1. **특징맵 다양성**: 더 많은 스케일로 다양한 크기 객체 감지
2. **백본 선택**: ResNet의 잔차 연결이 SSD에도 효과적
3. **앵커 설계**: 데이터셋 특성에 맞는 앵커 비율 조정 중요

### 7.3. 실용적 고려사항

**VGG16 모델 선택 시나리오:**
- 빠른 프로토타이핑 필요
- 제한된 개발 리소스
- 안정적 성능 우선

**ResNet34 모델 선택 시나리오:**
- 최고 성능 필요
- 커스터마이징 요구
- 충분한 개발 시간 보장

### 7.4. 향후 개선 방향

1. **데이터 증강**: 더 다양한 증강 기법 적용
2. **하이퍼파라미터 튜닝**: Grid Search 또는 베이지안 옵티마이제이션(Bayesian Optimization)
3. **앙상블**: 두 모델의 예측 결과 결합
4. **후처리**: NMS 파라미터 최적화

---

## 8. 용어 목록

| 용어 | 영문 | 설명 |
|------|------|------|
| 객체 감지 | Object Detection | 이미지에서 객체의 위치와 클래스를 동시에 예측하는 컴퓨터 비전 기술 |
| 단일 샷 감지 | Single Shot Detection | 한 번의 순전파로 객체 감지를 수행하는 방식 |
| 특징맵 | Feature Map | 컨볼루션 레이어의 출력으로 생성되는 특징 표현 |
| 앵커 박스 | Anchor Box / Default Box | 객체 감지를 위해 미리 정의된 참조 박스들 |
| 평균 정밀도 | Mean Average Precision, mAP | 객체 감지 모델의 성능을 측정하는 표준 지표 |
| 교집합 비율 | Intersection over Union, IoU | 예측 박스와 실제 박스 간의 겹치는 정도를 나타내는 지표 |
| 전이 학습 | Transfer Learning | 사전 훈련된 모델을 새로운 작업에 적용하는 학습 방법 |
| 백본 네트워크 | Backbone Network | 특징 추출을 담당하는 네트워크의 주요 부분 |
| 비최대 억제 | Non-Maximum Suppression, NMS | 중복된 감지 결과를 제거하는 후처리 기법 |
| 다중 스케일 | Multi-Scale | 서로 다른 크기의 특징맵을 활용하는 방식 |
| 어스펙트 레이셔 | Aspect Ratio | 박스의 가로세로 비율 |
| 잔차 연결 | Residual Connection | ResNet에서 사용되는 스킵 연결 구조 |
| 희석 컨볼루션 | Dilated Convolution | 필터 사이에 간격을 두어 수용 영역을 확장하는 컨볼루션 |
| 회귀 헤드 | Regression Head | 바운딩 박스 좌표를 예측하는 네트워크 부분 |
| 분류 헤드 | Classification Head | 객체 클래스를 예측하는 네트워크 부분 |