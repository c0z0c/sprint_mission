{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터셋 크기  100\n",
      "테스트데이터셋 크기  10000\n",
      "torch.Size([1, 28, 28])\n",
      "학습데이터셋 크기  1\n",
      "테스트데이터셋 크기  311\n",
      "Training start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dd8116013f4fef9fe6041fb1865196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fefbe9dd0614fc19bda020df9c595f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/5, val loss: 2.2712, val acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4716b6efa5f84396b2bd9e08eb0d982c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adb60f7f960479b83e9b44bd7e73f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2/5, val loss: 2.2622, val acc: 0.1400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4889fbff1b72419eab78708be9f5f5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bd7fb870a84a1ca6684a7b5a4ebc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3/5, val loss: 2.2566, val acc: 0.1400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fe6066942c44658140a0920804e8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756c112a0dae4b22a8de842eec0e8f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4/5, val loss: 2.2312, val acc: 0.1400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d3e603ee9546e78d5c74e79cb95294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515d0b1079ab4dac9e46512afb6711ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5/5, val loss: 2.1191, val acc: 0.1800\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import random_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def outsize(self, input_size, kernel_size, padding, stride):\n",
    "        return math.floor(input_size + 2 * padding - (kernel_size -1) -1) / stride + 1\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(3136, 64)\n",
    "        self.linear2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        output = self.linear2(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, leave=False, disable=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        max_scores, predicted = torch.max(preds, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, leave=False, disable=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            max_scores, predicted = torch.max(preds, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_loss = running_loss / total\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "\n",
    "train_tensor = torchvision.datasets.FashionMNIST(root='./fashion_mnist', train=True, download=True,\n",
    "                                            transform=v2.ToTensor())\n",
    "test_tensor = torchvision.datasets.FashionMNIST(root='./fashion_mnist', train=False, download=True,\n",
    "                                           transform=v2.ToTensor())\n",
    "\n",
    "train_imgs = torch.stack([img for img, _ in train_tensor], dim=0)\n",
    "#train_mean = train_imgs.mean(dim=(0, 2, 3)).tolist() # RGB\n",
    "#train_std = train_imgs.std(dim=(0, 2, 3)).tolist() # RGB\n",
    "train_mean = [train_imgs.mean().item()]\n",
    "train_std = [train_imgs.std().item()]\n",
    "\n",
    "transform_train = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomResizedCrop(size=28),\n",
    "        v2.RandomRotation(degrees=10),\n",
    "        v2.ToDtype(dtype=torch.float32, scale=True),\n",
    "        v2.Normalize(mean=train_mean, std=train_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(dtype=torch.float32, scale=True),\n",
    "        v2.Normalize(mean=train_mean, std=train_std)\n",
    "    ]\n",
    ")\n",
    " \n",
    "def data_load():\n",
    "    train_data = datasets.FashionMNIST(root='./fashion_mnist', train=True, download=True, transform=transform_train)\n",
    "    test_data = datasets.FashionMNIST(root='./fashion_mnist', train=False, download=True, transform=transform_test)\n",
    "    train_data = torch.utils.data.Subset(train_data, range(100)) # 테스트 100개\n",
    "\n",
    "    print('학습데이터셋 크기 ', len(train_data))\n",
    "    print('테스트데이터셋 크기 ', len(test_data))\n",
    "\n",
    "    img, label = train_data[0]\n",
    "    print(img.shape)  # (채널, 높이, 너비)\n",
    "    \n",
    "    train_dataset = train_data\n",
    "    #val_dataset, test_dataset = random_split(test_data, [5000, 5000])\n",
    "    val_dataset, test_dataset = random_split(test_data, [50, len(test_data)-50]) # 테스트 50개\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = data_load()\n",
    "print('학습데이터셋 크기 ', len(train_dataloader))\n",
    "print('테스트데이터셋 크기 ', len(test_dataloader))\n",
    "\n",
    "model = CNNModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "print('Training start!')\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_dataloader, loss_fn, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_dataloader, loss_fn, device)\n",
    "    print(f'epoch {epoch+1}/{epochs}, val loss: {val_loss:.4f}, val acc: {val_acc:.4f}')\n",
    "    \n",
    "print('Training finished!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py310env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
