{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbh70cn9M4l0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "훈련 데이터: 100개\n",
      "검증 데이터: 100개\n",
      "테스트 데이터: 100개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train [1/5]: 100%|██████████| 4/4 [00:34<00:00,  8.67s/it]\n",
      "Val   [1/5]: 100%|██████████| 4/4 [00:44<00:00, 11.04s/it]\n",
      "Train [2/5]: 100%|██████████| 4/4 [00:39<00:00,  9.84s/it] 종료 2025-08-18 21:20:37 KST]\n",
      "Val   [2/5]: 100%|██████████| 4/4 [00:19<00:00,  4.86s/it]\n",
      "Train [3/5]: 100%|██████████| 4/4 [00:23<00:00,  5.78s/it] 종료 2025-08-18 21:20:37 KST]\n",
      "Val   [3/5]: 100%|██████████| 4/4 [00:20<00:00,  5.21s/it]\n",
      "Train [4/5]: 100%|██████████| 4/4 [00:21<00:00,  5.28s/it] 종료 2025-08-18 21:20:37 KST]\n",
      "Val   [4/5]: 100%|██████████| 4/4 [00:19<00:00,  4.91s/it]\n",
      "Train [5/5]: 100%|██████████| 4/4 [00:24<00:00,  6.15s/it] 종료 2025-08-18 21:20:37 KST]\n",
      "Val   [5/5]: 100%|██████████| 4/4 [00:21<00:00,  5.40s/it]\n",
      "Epoch [5/5]: 100%|██████████| 5/5 [04:28<00:00, 53.76s/it, 종료 2025-08-18 21:20:37 KST]\n",
      "Testing: 100%|██████████| 4/4 [00:16<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 268.79 seconds\n",
      "test acc: 58.00\n",
      "class 0: 4.0/10.0 (40.00%)\n",
      "class 1: 4.0/6.0 (66.67%)\n",
      "class 2: 0.0/8.0 (0.00%)\n",
      "class 3: 7.0/10.0 (70.00%)\n",
      "class 4: 1.0/7.0 (14.29%)\n",
      "class 5: 3.0/8.0 (37.50%)\n",
      "class 6: 14.0/16.0 (87.50%)\n",
      "class 7: 6.0/11.0 (54.55%)\n",
      "class 8: 11.0/13.0 (84.62%)\n",
      "class 9: 8.0/11.0 (72.73%)\n",
      "best acc: 0.47% at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AlexNet                                  [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 256, 6, 6]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n",
       "│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n",
       "│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n",
       "│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n",
       "│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n",
       "│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n",
       "│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n",
       "│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n",
       "│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n",
       "│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n",
       "│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n",
       "├─Sequential: 1-3                        [1, 10]                   --\n",
       "│    └─Dropout: 2-14                     [1, 9216]                 --\n",
       "│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n",
       "│    └─ReLU: 2-16                        [1, 4096]                 --\n",
       "│    └─Dropout: 2-17                     [1, 4096]                 --\n",
       "│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n",
       "│    └─ReLU: 2-19                        [1, 4096]                 --\n",
       "│    └─Linear: 2-20                      [1, 10]                   40,970\n",
       "==========================================================================================\n",
       "Total params: 57,044,810\n",
       "Trainable params: 57,044,810\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 710.63\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 3.95\n",
       "Params size (MB): 228.18\n",
       "Estimated Total Size (MB): 232.73\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as v2\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import copy\n",
    "import pytz  # 시간대 처리\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "__kst = pytz.timezone('Asia/Seoul')\n",
    "__device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# 데이터 전처리 및 증강\n",
    "def get_data_transforms():\n",
    "    # 훈련용 데이터 전처리 (데이터 증강 포함)\n",
    "    train_transform = v2.Compose([\n",
    "        v2.Resize((224, 224)),  # AlexNet 입력 크기에 맞게 리사이즈\n",
    "        v2.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225])  # ImageNet 평균/표준편차\n",
    "    ])\n",
    "\n",
    "    # 검증/테스트용 데이터 전처리\n",
    "    val_transform = v2.Compose([\n",
    "        v2.Resize((224, 224)),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_transform, val_transform\n",
    "\n",
    "# 데이터셋 로드\n",
    "def load_data():\n",
    "    train_transform, val_transform = get_data_transforms() # 전처리\n",
    "\n",
    "    # CIFAR-10 데이터셋 다운로드 및 로드\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=train_transform)\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=val_transform)\n",
    "\n",
    "    # 검증 세트 생성 (훈련 데이터의 20% 사용)\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size])\n",
    "\n",
    "    # 검증 데이터셋에 올바른 transform 적용\n",
    "    val_dataset.dataset = copy.deepcopy(train_dataset.dataset)\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "\n",
    "    # DEBUG_ON이 True면 3개만 사용\n",
    "    if True:\n",
    "        from torch.utils.data import Subset\n",
    "        train_dataset = Subset(train_dataset, range(100))\n",
    "        val_dataset = Subset(val_dataset, range(100))\n",
    "        test_dataset = Subset(test_dataset, range(100))\n",
    "        batch_size = 32\n",
    "    else:\n",
    "        batch_size = 32\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                          shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                           shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f\"훈련 데이터: {len(train_dataset)}개\")\n",
    "    print(f\"검증 데이터: {len(val_dataset)}개\")\n",
    "    print(f\"테스트 데이터: {len(test_dataset)}개\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# 훈련 함수\n",
    "def train_fn(model, train_loader, criterion, optimizer, device, epoch=None, epochs=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    res = {}\n",
    "    desc = f\"Train [{epoch+1}/{epochs}]\" if epoch is not None and epochs is not None else \"Train\"\n",
    "    tqdm_kwargs = {} \n",
    "    tqdm_kwargs['disable'] = False\n",
    "    tqdm_kwargs['mininterval'] = 1\n",
    "    pbar = tqdm(train_loader, leave=True, desc=desc, position=0, **tqdm_kwargs)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        # criterion이 기본(reduction='mean')이라 가정하여 샘플 기준 누적\n",
    "        running_loss += loss.item() * batch_size\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_size\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total if total > 0 else 0.0\n",
    "    epoch_acc = correct / total if total > 0 else 0.0\n",
    "    res['loss'] = epoch_loss\n",
    "    res['acc'] = epoch_acc\n",
    "    return res\n",
    "\n",
    "# 검증 함수\n",
    "def evaluate_fn(model, val_loader, criterion, device, epoch, epochs):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    res = {}\n",
    "    desc = f\"Val   [{epoch+1}/{epochs}]\"\n",
    "    tqdm_kwargs = {} \n",
    "    tqdm_kwargs['disable'] = False\n",
    "    tqdm_kwargs['mininterval'] = 1\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, leave=True, desc=desc, position=0, **tqdm_kwargs)\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_size\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total if total > 0 else 0.0\n",
    "    epoch_acc = correct / total if total > 0 else 0.0\n",
    "    res['loss'] = epoch_loss\n",
    "    res['acc'] = epoch_acc\n",
    "    return res\n",
    "\n",
    "# 모델링 함수 (훈련 + 검증)\n",
    "def modeling_fn(model, epochs, train_loader, val_loader, criterion, optimizer, scheduler, device):\n",
    "    import sys\n",
    "    history = []\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    " \n",
    "    start_time = time.time()\n",
    "    epoch_start_time = None\n",
    "\n",
    "    estimated_completion_str = None\n",
    "    \n",
    "    tqdm_kwargs = {} \n",
    "    tqdm_kwargs['disable'] = False\n",
    "    tqdm_kwargs['mininterval'] = 1\n",
    "    epoch = 0\n",
    "    desc = f\"Epoch [{epoch+1}/{epochs}]\"\n",
    "    pbar = tqdm(range(epochs), leave=True, desc=desc, position=0, **tqdm_kwargs)\n",
    "    for epoch in pbar:\n",
    "        sys.stdout.flush()  # 출력 버퍼 비우기\n",
    "        epoch_start_time = time.time()\n",
    "        pbar.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        \n",
    "        train_res = train_fn(model, train_loader, criterion, optimizer, device, epoch=epoch, epochs=epochs)\n",
    "        val_res = evaluate_fn(model, val_loader, criterion, device, epoch=epoch, epochs=epochs)\n",
    "        if estimated_completion_str is not None:\n",
    "            pbar.set_postfix_str(\"종료 \" + estimated_completion_str)\n",
    "            \n",
    "        # 스케줄러 업데이트\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        res = {\n",
    "            \"epoch\": epoch,\n",
    "            \"epochs\": epochs,\n",
    "            \"train\": train_res,\n",
    "            \"val\": val_res,\n",
    "        }\n",
    "\n",
    "        history.append(res)\n",
    "\n",
    "        # 최고 성능 모델 저장\n",
    "        if val_res['acc'] > best_val_acc:\n",
    "            best_val_acc = val_res['acc']\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # 에포크 완료 시간 계산\n",
    "        epoch_elapsed = time.time() - epoch_start_time\n",
    "\n",
    "        # 첫 번째 에포크 완료 후 예상 완료 시간 계산\n",
    "        if epoch == 0 and epochs > 1:\n",
    "            remaining_epochs = epochs - 1\n",
    "            estimated_total_time = epoch_elapsed * epochs\n",
    "            estimated_completion = datetime.now(__kst) + timedelta(seconds=epoch_elapsed * remaining_epochs)\n",
    "            estimated_completion_str = estimated_completion.strftime('%Y-%m-%d %H:%M:%S KST')\n",
    "            pbar.set_postfix_str(\"종료 \" + estimated_completion_str)\n",
    "            \n",
    "    total_elapsed = time.time() - start_time\n",
    "    completion_time = datetime.now(__kst)\n",
    "\n",
    "    # 최고 성능 모델 가중치 로드\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history, completion_time, total_elapsed\n",
    "\n",
    "# 테스트 함수\n",
    "def test_model(model, test_loader, device):\n",
    "    \"\"\"모델 테스트\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, leave=True, desc=\"Testing\", position=0)\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # 클래스별 정확도 계산\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, class_total, classes, class_correct\n",
    "\n",
    "def running():\n",
    "    train_loader, val_loader, test_loader = load_data()\n",
    "    # 먼저 사전 훈련된 모델을 1000 클래스로 로드\n",
    "    model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT, progress=True)\n",
    "    \n",
    "    # 마지막 분류 레이어를 CIFAR-10에 맞게 교체 (1000 -> 10)\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    model = model.to(__device)\n",
    "    \n",
    "    # 손실 함수와 옵티마이저 설정\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    # 모델 학습\n",
    "    model, history, completion_time, total_elapsed = modeling_fn(model, 5, train_loader, val_loader, criterion, optimizer, scheduler, __device)\n",
    "    best = max(history, key=lambda x: x['val']['acc'])\n",
    "\n",
    "    test_acc, test_class_total, test_classes, test_class_correct = test_model(model, test_loader, __device)\n",
    "\n",
    "    print(f\"total : {total_elapsed:.2f} seconds\")\n",
    "    print(f\"test acc: {test_acc:.2f}\")\n",
    "    for i in range(10):\n",
    "        print(f\"class {i}: {test_class_correct[i]}/{test_class_total[i]} ({100 * test_class_correct[i] / test_class_total[i]:.2f}%)\")\n",
    "    print(f\"best acc: {best['val']['acc']:.2f}% at epoch {best['epoch']+1}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = running()\n",
    "\n",
    "# 모델 학습\n",
    "from torchinfo import summary\n",
    "summary(res['model'], input_size = (1, 3, 224, 224 ))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py310env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
