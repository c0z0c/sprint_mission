{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQv15fF-MfKH"
   },
   "source": [
    "# Amazon Food Review 요약 실습\n",
    "- 데이터셋 링크\n",
    "    https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "    \n",
    "- 실습개요\n",
    "• 긴 텍스트를 짧은 요약으로 만들어주는 AI 만들기\" Amazon 리뷰 데이터를 사용해서 긴 리뷰 텍스트를 짧은 한 줄 요약으로 바꿔주는 모델을 만드는 실습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ 현실에서는\n",
    "\n",
    "**Hugging Face** 같은 라이브러리 사용\n",
    "\n",
    "- **미리 훈련된 모델** 활용 (BERT, GPT 등)\n",
    "- **Fine-tuning**으로 우리 데이터에 맞게 조정 →CNN에서의 전이학습 튜닝\n",
    "\n",
    "→ 25년도8월에서는  RAG가 대세(에이전트화): 도메인이해, 청크사이즈(어떤 단위로 나뉘어서 기억할것인가?), 효율적인 전처리, 벡터DB이용\n",
    "  원본소스를 불러오는 과정이 어려움  (원본:텍스트로만, 표로, 이미지텍스트(OCR), pdf, 웹문서, 미술관옆동물원과같은 html문서등) —>파이썬 코드로 해결\n",
    "\n",
    ".→ 학습결과 비교\n",
    "\n",
    "Seq2Seq:   10 에포크 → Loss: 0.2826\n",
    "Attention: 10 에포크 → Loss: 0.2779  (약간 개선)\n",
    "Transformer: 7 에포크 → Loss: 3.0384 (빠른 수렴)\n",
    "\n",
    "→ 생성품질비교\n",
    "\n",
    "Seq2Seq: \"Great\", \"Great\", \"Great\"... (단조로움)\n",
    "Attention: \"Great\", \"Not for\", \"Great tea\"... (조금 다양)\n",
    "Transformer: 더 복잡한 패턴 학습 가능 (에포크가 적어서 결과 미완성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve; urlretrieve(\"https://raw.githubusercontent.com/c0z0c/jupyter_hangul/refs/heads/beta/helper_utils.py\", \"helper_utils.py\")\n",
    "import importlib\n",
    "import helper_utils as hu\n",
    "importlib.reload(hu)\n",
    "from helper_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12359,
     "status": "ok",
     "timestamp": 1756211969876,
     "user": {
      "displayName": "dev c0z0c",
      "userId": "08071297324787696567"
     },
     "user_tz": -540
    },
    "id": "TXoYK8FtMfKM",
    "outputId": "6f0c44f4-2d41-4305-9fcf-d9ce33767492"
   },
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "\n",
    "# --- Scikit-learn: 데이터 전처리, 모델, 평가 ---\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import (\n",
    "    fetch_california_housing, load_iris, make_moons, make_circles,\n",
    "    load_breast_cancer, load_wine\n",
    ")\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# --- 기타 라이브러리 ---\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "from PIL import ImageDraw\n",
    "import albumentations as A\n",
    "import IPython.display\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- PyTorch: 딥러닝 관련 ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from collections import OrderedDict\n",
    "\n",
    "# --- 기타 ---\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timezone, timedelta\n",
    "import pytz\n",
    "__kst = pytz.timezone('Asia/Seoul')\n",
    "\n",
    "# GPU 설정\n",
    "__device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "__device_cpu = torch.device('cpu')\n",
    "\n",
    "  # 재현 가능한 결과를 위해\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if __device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"라이브러리 로드 완료 사용장치:{__device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이타 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"snap/amazon-fine-food-reviews\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "print_dir_tree(path)\n",
    "\n",
    "data_csv_path = os.path.join(path, \"Reviews.csv\")\n",
    "data_jsonl_path = os.path.join(path, \"Reviews.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이타 jsonl 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_jsonl(input_file, output_file):\n",
    "    \"\"\"\n",
    "    CSV 파일을 읽어 Hugging Face 데이터셋에 적합한 JSON Lines(.jsonl) 형식으로 변환합니다.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): 입력 CSV 파일 경로.\n",
    "        output_file (str): 출력 JSON Lines 파일 경로.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 예시 데이터 로드: io.StringIO를 사용하여 문자열 데이터를 파일처럼 처리\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        # 2. NaN(결측값)을 빈 문자열로 처리 (필요에 따라 선택)\n",
    "        # 딥러닝 학습 시 빈 값이 오류를 일으킬 수 있으므로 미리 처리합니다.\n",
    "        df = df.fillna(\"\")\n",
    "\n",
    "        df.to_json(\n",
    "            output_file,\n",
    "            orient='records',\n",
    "            lines=True,\n",
    "            force_ascii=False\n",
    "        )\n",
    "        print(f\"✅ 변환 성공: '{output_file}' 파일이 생성되었습니다.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 오류: 입력 파일 '{input_file}'을 찾을 수 없습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 변환 중 오류 발생: {e}\")\n",
    "\n",
    "\n",
    "if not os.path.exists(data_jsonl_path):\n",
    "    csv_to_jsonl(data_csv_path, data_jsonl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이타 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_json = load_dataset(\"json\", data_files=data_jsonl_path)\n",
    "\n",
    "print(raw_json)\n",
    "print(\"-\" * 30)\n",
    "print('raw_datasets', type(raw_json))\n",
    "print(raw_json['train'][0])\n",
    "\n",
    "dataset_org = raw_json['train']\n",
    "print('train_dataset', type(dataset_org))\n",
    "print('train_dataset[0]', dataset_org[0])\n",
    "\n",
    "import pandas as pd\n",
    "df_org = dataset_org.to_pandas()\n",
    "# 기본정보\n",
    "print(df_org.shape)\n",
    "print(df_org.columns.tolist())\n",
    "print(df_org.isna().sum())\n",
    "df_org.head_att(3)\n",
    "df = df_org.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이타 분석 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "# df가 존재한다고 가정 (df = df_org.copy() 상태)\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 기본 복사 안전 처리\n",
    "df = df_org.copy()\n",
    "\n",
    "# 타임스탬프 -> datetime, 길이 정보 추가\n",
    "df['datetime'] = pd.to_datetime(df['Time'], unit='s', errors='coerce')\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['text_char_len'] = df['Text'].astype(str).str.len()\n",
    "df['text_word_len'] = df['Text'].astype(str).str.split().map(len)\n",
    "df['summary_char_len'] = df['Summary'].astype(str).str.len()\n",
    "df['summary_word_len'] = df['Summary'].astype(str).str.split().map(len)\n",
    "\n",
    "# 출력: 상위 정보 테이블\n",
    "print(\"데이터개요:\", df.shape)\n",
    "display(df.head(5))\n",
    "print(\"\\n컬럼별 결측수:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Score 분포\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Score', data=df, palette='crest')\n",
    "plt.title('Score 분포')\n",
    "plt.show()\n",
    "\n",
    "# 텍스트/요약 길이 분포 (히스토그램 + 박스플롯)\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,8))\n",
    "sns.histplot(df['text_word_len'], bins=50, ax=axes[0,0], kde=True)\n",
    "axes[0,0].set_title('Text word length 분포')\n",
    "sns.boxplot(x='Score', y='text_word_len', data=df.sample(20000, random_state=1), ax=axes[0,1])\n",
    "axes[0,1].set_title('Score별 Text word length (샘플)')\n",
    "\n",
    "sns.histplot(df['summary_word_len'], bins=40, ax=axes[1,0], kde=True)\n",
    "axes[1,0].set_title('Summary word length 분포')\n",
    "sns.boxplot(x='Score', y='summary_word_len', data=df.sample(20000, random_state=2), ax=axes[1,1])\n",
    "axes[1,1].set_title('Score별 Summary word length (샘플)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 연도별 리뷰 건수 추이 (상위 연도만)\n",
    "cnt_by_year = df['year'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8,3))\n",
    "cnt_by_year.plot(kind='bar')\n",
    "plt.title('연도별 리뷰 수')\n",
    "plt.xlabel('year')\n",
    "plt.show()\n",
    "print(\"연도별 리뷰 수 (상위):\")\n",
    "display(cnt_by_year.tail(10))\n",
    "\n",
    "# Score별 텍스트 길이 통계\n",
    "print(\"\\nScore별 Text 길이 통계 (word count):\")\n",
    "display(df.groupby('Score')['text_word_len'].agg(['count','mean','median','std']).sort_index())\n",
    "\n",
    "# 상위 ProductId, UserId\n",
    "print(\"\\nTop ProductId (리뷰 수 상위 10):\")\n",
    "display(df['ProductId'].value_counts().head(10))\n",
    "print(\"\\nTop UserId (리뷰 수 상위 10):\")\n",
    "display(df['UserId'].value_counts().head(10))\n",
    "\n",
    "# 샘플 리뷰 (Score별 1개씩)\n",
    "print(\"\\nScore별 샘플 Summary / Text:\")\n",
    "for s in sorted(df['Score'].unique()):\n",
    "    row = df[df['Score']==s].iloc[0]\n",
    "    print(f\"\\nScore={s} Summary: {row['Summary'][:120]}\")\n",
    "    print(f\"         Text (first 200 chars): {str(row['Text'])[:200]}\")\n",
    "\n",
    "# 텍스트 상위 토큰 (영어 불용어 제외)\n",
    "cv = CountVectorizer(stop_words='english', max_features=40, token_pattern=r\"(?u)\\b[a-zA-Z']+\\b\")\n",
    "X = cv.fit_transform(df['Text'].astype(str).sample(min(50000, len(df)), random_state=0))\n",
    "tok_counts = np.array(X.sum(axis=0)).ravel()\n",
    "tok_idx = tok_counts.argsort()[::-1]\n",
    "tokens = np.array(cv.get_feature_names_out())[tok_idx]\n",
    "counts = tok_counts[tok_idx]\n",
    "top_tokens = pd.DataFrame({'token': tokens, 'count': counts}).head(30)\n",
    "print(\"\\n상위 토큰 (샘플 50k 문서 기준, stop_words='english'):\")\n",
    "display(top_tokens)\n",
    "\n",
    "# 상위 Summary 길이/예외 샘플 확인 (짧거나 긴 케이스)\n",
    "print(\"\\n가장 짧은 Summary 예시:\")\n",
    "display(df.nsmallest(3, 'summary_word_len')[['Score','Summary','Summary','summary_word_len']])\n",
    "print(\"\\n가장 긴 Summary 예시:\")\n",
    "display(df.nlargest(3, 'summary_word_len')[['Score','Summary','summary_word_len']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이타 전처리 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "def full_preprocessing_pipeline(jsonl_path):\n",
    "    \"\"\"\n",
    "    강화된 전처리 파이프라인 (리팩토링 버전)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Amazon Fine Food Reviews 전처리 시작\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    print(\"\\n1. 데이터 로드...\")\n",
    "    raw_datasets = load_dataset(\"json\", data_files=jsonl_path)\n",
    "    dataset = raw_datasets['train']\n",
    "    print(f\"   원본: {len(dataset):,}개\")\n",
    "    \n",
    "    # 2. 필요한 컬럼만 선택\n",
    "    print(\"\\n2. 컬럼 선택 (Text, Summary)...\")\n",
    "    dataset = dataset.select_columns(['Text', 'Summary'])\n",
    "    \n",
    "    # 3. 텍스트 정제\n",
    "    print(\"\\n3. 텍스트 정제...\")\n",
    "    \n",
    "    def clean_text(text):\n",
    "        \"\"\"리팩토링된 텍스트 정제 함수\"\"\"\n",
    "        \n",
    "        # None 또는 빈 문자열 체크\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # ==================== 1. HTML 태그 제거 ====================\n",
    "        html_patterns = [\n",
    "            (r'<br\\s*/?\\s*>', ' ', re.IGNORECASE),  # <br> 변형들\n",
    "            (r'<[^>]+>', '', 0),                     # 모든 HTML 태그\n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement, flags in html_patterns:\n",
    "            if flags:\n",
    "                text = re.sub(pattern, replacement, text, flags=flags)\n",
    "            else:\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # ==================== 2. HTML 엔티티 디코딩 ====================\n",
    "        # 중첩된 엔티티 대비 여러 번 수행\n",
    "        for _ in range(3):\n",
    "            text = html.unescape(text)\n",
    "        \n",
    "        # 남은 HTML 엔티티 강제 제거\n",
    "        entity_patterns = [\n",
    "            r'&[a-zA-Z]+;',          # &nbsp; &amp; 등\n",
    "            r'&#\\d+;',               # &#123; 등\n",
    "            r'&#x[0-9a-fA-F]+;',     # &#xAB; 등\n",
    "        ]\n",
    "        \n",
    "        for pattern in entity_patterns:\n",
    "            text = re.sub(pattern, '', text)\n",
    "        \n",
    "        # ==================== 3. URL/이메일 제거 ====================\n",
    "        web_patterns = [\n",
    "            r'http[s]?://\\S+',       # http:// https://\n",
    "            r'www\\.\\S+',             # www.\n",
    "            r'\\S+@\\S+\\.\\S+',         # 이메일\n",
    "        ]\n",
    "        \n",
    "        for pattern in web_patterns:\n",
    "            text = re.sub(pattern, '', text)\n",
    "        \n",
    "        # ==================== 4. 유니코드 정규화 (Non-ASCII 제거) ====================\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        \n",
    "        # ==================== 5. 제어 문자 제거 ====================\n",
    "        text = ''.join(c if ord(c) >= 32 or c in '\\t\\n\\r' else ' ' for c in text)\n",
    "        \n",
    "        # ==================== 6. 소문자 변환 ====================\n",
    "        text = text.lower()\n",
    "        \n",
    "        # ==================== 7. 반복 특수문자 정규화 ====================\n",
    "        repeated_punct_patterns = [\n",
    "            (r'\\.{2,}', '.'),        # ... → .\n",
    "            (r'!{2,}', '!'),         # !!! → !\n",
    "            (r'\\?{2,}', '?'),        # ??? → ?\n",
    "            (r'-{2,}', '-'),         # --- → -\n",
    "            (r'_{2,}', '_'),         # ___ → _\n",
    "            (r'={2,}', '='),         # === → =\n",
    "            (r'\\*{2,}', '*'),        # *** → *\n",
    "            (r'\\+{2,}', '+'),        # +++ → +\n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement in repeated_punct_patterns:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # ==================== 8. 따옴표 정규화 ====================\n",
    "        quote_replacements = [\n",
    "            ('\"', '\"'), ('\"', '\"'),  # 스마트 쿼트\n",
    "            (''', \"'\"), (''', \"'\"),  # 스마트 어포스트로피\n",
    "            ('`', \"'\"), ('´', \"'\"),  # 백틱, 악센트\n",
    "        ]\n",
    "        \n",
    "        for old, new in quote_replacements:\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # ==================== 9. 대시/하이픈 정규화 ====================\n",
    "        dash_replacements = [\n",
    "            ('—', '-'),  # em dash\n",
    "            ('–', '-'),  # en dash\n",
    "            ('―', '-'),  # horizontal bar\n",
    "        ]\n",
    "        \n",
    "        for old, new in dash_replacements:\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # ==================== 10. 특수 공백 정규화 ====================\n",
    "        space_replacements = [\n",
    "            ('\\n', ' '),           # 줄바꿈\n",
    "            ('\\t', ' '),           # 탭\n",
    "            ('\\r', ' '),           # 캐리지 리턴\n",
    "            ('\\xa0', ' '),         # non-breaking space\n",
    "            ('\\u2009', ' '),       # thin space\n",
    "            ('\\u200b', ''),        # zero-width space\n",
    "            ('\\u200c', ''),        # zero-width non-joiner\n",
    "            ('\\u200d', ''),        # zero-width joiner\n",
    "            ('\\ufeff', ''),        # zero-width no-break space (BOM)\n",
    "        ]\n",
    "        \n",
    "        for old, new in space_replacements:\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # ==================== 11. 연속 공백 정규화 ====================\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # ==================== 12. 양쪽 공백/특수문자 제거 ====================\n",
    "        text = text.strip()\n",
    "        text = text.strip('-._,;:!?')\n",
    "        text = text.strip()\n",
    "        \n",
    "        # ==================== 13. 최종 품질 검증 ====================\n",
    "        if text and len(text) > 0:\n",
    "            # 알파벳+공백 비율이 너무 낮으면 버림\n",
    "            alpha_space_ratio = sum(c.isalpha() or c.isspace() for c in text) / len(text)\n",
    "            if alpha_space_ratio < 0.5:\n",
    "                return \"\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        texts = [clean_text(t) for t in examples['Text']]\n",
    "        summaries = [clean_text(s) for s in examples['Summary']]\n",
    "        text_lengths = [len(t.split()) if t else 0 for t in texts]\n",
    "        summary_lengths = [len(s.split()) if s else 0 for s in summaries]\n",
    "        \n",
    "        return {\n",
    "            'Text': texts,\n",
    "            'Summary': summaries,\n",
    "            'text_length': text_lengths,\n",
    "            'summary_length': summary_lengths\n",
    "        }\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        desc=\"텍스트 정제\",\n",
    "    )\n",
    "    \n",
    "    # 4. 길이 필터링\n",
    "    print(\"\\n4. 길이 필터링...\")\n",
    "    print(f\"   필터링 전: {len(dataset):,}개\")\n",
    "    \n",
    "    def filter_by_length(example):\n",
    "        return (10 <= example['text_length'] <= 500 and \n",
    "                1 <= example['summary_length'] <= 50)\n",
    "    \n",
    "    dataset = dataset.filter(filter_by_length, desc=\"길이 필터링\")\n",
    "    print(f\"   필터링 후: {len(dataset):,}개\")\n",
    "    \n",
    "    # 5. 품질 필터링\n",
    "    print(\"\\n5. 품질 필터링...\")\n",
    "    print(f\"   필터링 전: {len(dataset):,}개\")\n",
    "    \n",
    "    def filter_quality(example):\n",
    "        text = example['Text']\n",
    "        summary = example['Summary']\n",
    "        \n",
    "        # 빈 문자열 체크\n",
    "        if not text or not summary or text.strip() == '' or summary.strip() == '':\n",
    "            return False\n",
    "        \n",
    "        # 품질 체크 패턴 리스트\n",
    "        quality_checks = [\n",
    "            # (체크할 텍스트, 패턴, 설명)\n",
    "            (text, r'<[^>]+>', 'HTML 태그'),\n",
    "            (summary, r'<[^>]+>', 'HTML 태그'),\n",
    "            (text, r'&[a-zA-Z]+;', 'HTML 엔티티'),\n",
    "            (summary, r'&[a-zA-Z]+;', 'HTML 엔티티'),\n",
    "        ]\n",
    "        \n",
    "        for check_text, pattern, _ in quality_checks:\n",
    "            if re.search(pattern, check_text):\n",
    "                return False\n",
    "        \n",
    "        # 제어 문자 체크\n",
    "        for check_text in [text, summary]:\n",
    "            if any(ord(c) < 32 and c not in '\\t\\n\\r' for c in check_text):\n",
    "                return False\n",
    "        \n",
    "        # 비정상 반복 체크 (같은 단어 5회 연속)\n",
    "        words = text.split()\n",
    "        if len(words) >= 5:\n",
    "            for i in range(len(words) - 4):\n",
    "                if len(set(words[i:i+5])) == 1:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    dataset = dataset.filter(filter_quality, desc=\"품질 필터링\")\n",
    "    print(f\"   필터링 후: {len(dataset):,}개\")\n",
    "    \n",
    "    # 6. 중복 제거\n",
    "    print(\"\\n6. 중복 제거...\")\n",
    "    print(f\"   제거 전: {len(dataset):,}개\")\n",
    "    \n",
    "    seen = set()\n",
    "    unique_indices = []\n",
    "    for idx, text in enumerate(dataset['Text']):\n",
    "        text_hash = hash(text)\n",
    "        if text_hash not in seen:\n",
    "            seen.add(text_hash)\n",
    "            unique_indices.append(idx)\n",
    "    \n",
    "    dataset = dataset.select(unique_indices)\n",
    "    print(f\"   제거 후: {len(dataset):,}개\")\n",
    "    \n",
    "    # 7. 통계 출력\n",
    "    print(\"\\n7. 통계:\")\n",
    "    text_lengths = dataset['text_length']\n",
    "    summary_lengths = dataset['summary_length']\n",
    "    \n",
    "    stats = [\n",
    "        (\"Text\", text_lengths),\n",
    "        (\"Summary\", summary_lengths)\n",
    "    ]\n",
    "    \n",
    "    for name, lengths in stats:\n",
    "        print(f\"   {name} 길이:\")\n",
    "        print(f\"      평균: {np.mean(lengths):.1f} 단어\")\n",
    "        print(f\"      중앙값: {np.median(lengths):.1f} 단어\")\n",
    "        print(f\"      최소: {np.min(lengths)} 단어\")\n",
    "        print(f\"      최대: {np.max(lengths)} 단어\")\n",
    "        print(f\"      표준편차: {np.std(lengths):.1f}\")\n",
    "        if name == \"Text\":\n",
    "            print()\n",
    "    \n",
    "    # 8. 데이터 분할\n",
    "    print(\"\\n8. 데이터 분할...\")\n",
    "    train_val_test = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_val = train_val_test['train']\n",
    "    test = train_val_test['test']\n",
    "    \n",
    "    train_val_split = train_val.train_test_split(test_size=0.111, seed=42)\n",
    "    train = train_val_split['train']\n",
    "    val = train_val_split['test']\n",
    "    \n",
    "    print(f\"   Train: {len(train):,}개 ({len(train)/len(dataset)*100:.1f}%)\")\n",
    "    print(f\"   Val: {len(val):,}개 ({len(val)/len(dataset)*100:.1f}%)\")\n",
    "    print(f\"   Test: {len(test):,}개 ({len(test)/len(dataset)*100:.1f}%)\")\n",
    "    \n",
    "    # 9. 최종 데이터셋\n",
    "    final_datasets = DatasetDict({\n",
    "        'train': train,\n",
    "        'validation': val,\n",
    "        'test': test\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"전처리 완료!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n최종 데이터셋:\")\n",
    "    print(final_datasets)\n",
    "    \n",
    "    return final_datasets\n",
    "\n",
    "\n",
    "# 전처리 및 저장\n",
    "dataset_path = os.path.join(drive_root(), 'data', \"AmazonFineFoodReviews.dataset\")   \n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    processed_datasets = full_preprocessing_pipeline(data_jsonl_path)\n",
    "    os.makedirs(os.path.dirname(dataset_path), exist_ok=True)\n",
    "    processed_datasets.save_to_disk(dataset_path)\n",
    "    print(f\"\\n저장완료: {dataset_path}\")\n",
    "else:\n",
    "    print(f\"이미 존재하는 데이터셋 경로입니다: {dataset_path}\")\n",
    "    print(\"기존 데이터를 삭제하고 새로 전처리하려면:\")\n",
    "    print(f\"   import shutil\")\n",
    "    print(f\"   shutil.rmtree('{dataset_path}')\")\n",
    "    print(f\"   # 그 다음 다시 실행\")\n",
    "    final_datasets = DatasetDict.load_from_disk(dataset_path)\n",
    "    print(final_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분할 데이타 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 로드\n",
    "dataset_path = os.path.join(drive_root(), 'data', \"AmazonFineFoodReviews.dataset\")\n",
    "final_datasets = DatasetDict.load_from_disk(dataset_path)\n",
    "\n",
    "print(final_datasets)\n",
    "print(\"\\n샘플 확인:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train 샘플 3개 출력\n",
    "for i in range(3):\n",
    "    sample = final_datasets['train'][i]\n",
    "    print(f\"\\n[샘플 {i+1}]\")\n",
    "    print(f\"Text ({sample['text_length']} 단어):\")\n",
    "    print(f\"  {sample['Text'][:200]}...\")\n",
    "    print(f\"\\nSummary ({sample['summary_length']} 단어):\")\n",
    "    print(f\"  {sample['Summary']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 길이 분포 시각화\n",
    "train_text_lengths = final_datasets['train']['text_length']\n",
    "train_summary_lengths = final_datasets['train']['summary_length']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Text 길이 분포\n",
    "axes[0].hist(train_text_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Text Length (words)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "axes[0].axvline(np.mean(train_text_lengths), color='r', linestyle='--', \n",
    "                label=f'Mean: {np.mean(train_text_lengths):.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Summary 길이 분포\n",
    "axes[1].hist(train_summary_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Summary Length (words)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Summary Length Distribution')\n",
    "axes[1].axvline(np.mean(train_summary_lengths), color='r', linestyle='--',\n",
    "                label=f'Mean: {np.mean(train_summary_lengths):.1f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ 시각화 저장 완료: data_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import os, re, pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# final_datasets가 없으면 디스크에서 로드\n",
    "try:\n",
    "    final_datasets\n",
    "except NameError:\n",
    "    dataset_path = os.path.join(drive_root(), 'data', \"AmazonFineFoodReviews.dataset\")\n",
    "    final_datasets = load_from_disk(dataset_path)\n",
    "\n",
    "train = final_datasets['train']\n",
    "n = len(train)\n",
    "print(f\"Train 샘플 수: {n:,}\")\n",
    "\n",
    "issues = []\n",
    "html_re = re.compile(r'<[^>]+>')\n",
    "entity_re = re.compile(r'&[a-zA-Z]+;')\n",
    "\n",
    "pbar = tqdm(range(n), total=n, desc=\"샘플 검사 중\")\n",
    "for i in pbar:\n",
    "    row = train[i]\n",
    "    text = row.get('Text', '')\n",
    "    summary = row.get('Summary', '')\n",
    "\n",
    "    row_issues = []\n",
    "    # 타입/빈값\n",
    "    if not isinstance(text, str): row_issues.append('text_not_str')\n",
    "    if not isinstance(summary, str): row_issues.append('summary_not_str')\n",
    "    if isinstance(text, str) and text.strip() == '': row_issues.append('text_empty')\n",
    "    if isinstance(summary, str) and summary.strip() == '': row_issues.append('summary_empty')\n",
    "\n",
    "    # HTML / 엔티티 잔존\n",
    "    if isinstance(text, str) and (html_re.search(text) or entity_re.search(text)): row_issues.append('text_html_or_entity')\n",
    "    if isinstance(summary, str) and (html_re.search(summary) or entity_re.search(summary)): row_issues.append('summary_html_or_entity')\n",
    "\n",
    "    # 반복된 특수문자(!!!, ???, ...)\n",
    "    if isinstance(text, str) and re.search(r'([!?\\.])\\1{2,}', text): row_issues.append('text_repeated_punct')\n",
    "    if isinstance(summary, str) and re.search(r'([!?\\.])\\1{2,}', summary): row_issues.append('summary_repeated_punct')\n",
    "\n",
    "    # 제어문자\n",
    "    if isinstance(text, str) and any(ord(c) < 32 and c not in '\\t\\n\\r' for c in text): row_issues.append('text_ctrl_char')\n",
    "    if isinstance(summary, str) and any(ord(c) < 32 and c not in '\\t\\n\\r' for c in summary): row_issues.append('summary_ctrl_char')\n",
    "\n",
    "    # 길이 이상치 (단어 수)\n",
    "    if isinstance(text, str):\n",
    "        tw = len(text.split())\n",
    "        if tw < 5: row_issues.append(f'text_too_short({tw})')\n",
    "        if tw > 2000: row_issues.append(f'text_too_long({tw})')\n",
    "    if isinstance(summary, str):\n",
    "        sw = len(summary.split())\n",
    "        if sw < 1: row_issues.append(f'summary_too_short({sw})')\n",
    "        if sw > 200: row_issues.append(f'summary_too_long({sw})')\n",
    "\n",
    "    # 비정상 문자(비 ASCII) — 필요에 따라 제거/허용\n",
    "    if isinstance(text, str) and any(ord(c) > 127 for c in text): row_issues.append('text_non_ascii')\n",
    "    if isinstance(summary, str) and any(ord(c) > 127 for c in summary): row_issues.append('summary_non_ascii')\n",
    "\n",
    "    if row_issues:\n",
    "        issues.append({\n",
    "            'index': i,\n",
    "            'text_preview': (text[:200] if isinstance(text,str) else str(text)) ,\n",
    "            'summary_preview': (summary[:200] if isinstance(summary,str) else str(summary)),\n",
    "            'issues': ';'.join(row_issues)\n",
    "        })\n",
    "\n",
    "print(f\"문제 샘플 수: {len(issues):,}\")\n",
    "\n",
    "# 결과 출력 및 저장\n",
    "df_issues = pd.DataFrame(issues)\n",
    "display(df_issues.head(50))\n",
    "# out_path = os.path.join('data', 'train_issues.csv')\n",
    "# os.makedirs('data', exist_ok=True)\n",
    "# df_issues.to_csv(out_path, index=False, encoding='utf-8-sig')\n",
    "# print(f\"저장: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.DataFrame(issues)\n",
    "display(df_issues.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 무작위 샘플 10개 확인\n",
    "random_indices = random.sample(range(len(final_datasets['train'])), 5)\n",
    "\n",
    "print(\"무작위 샘플 5개 품질 확인:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx in random_indices:\n",
    "    sample = final_datasets['train'][idx]\n",
    "    \n",
    "    # 품질 체크\n",
    "    has_special_chars = any(c * 3 in sample['Text'] for c in '!?.') \n",
    "    has_html = '&' in sample['Text'] or '<' in sample['Text']\n",
    "    \n",
    "    print(f\"\\n[인덱스: {idx}]\")\n",
    "    print(f\"Text: {sample['Text'][:150]}...\")\n",
    "    print(f\"Summary: {sample['Summary']}\")\n",
    "    print(f\"길이: Text={sample['text_length']}, Summary={sample['summary_length']}\")\n",
    "    \n",
    "    # 품질 이슈 표시\n",
    "    issues = []\n",
    "    if has_special_chars:\n",
    "        issues.append(\"특수문자 반복\")\n",
    "    if has_html:\n",
    "        issues.append(\"HTML 잔존\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"이슈: {', '.join(issues)}\")\n",
    "    else:\n",
    "        print(\"양호\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import BartTokenizer\n",
    "import os\n",
    "\n",
    "# 데이터 로드\n",
    "dataset_path = os.path.join(drive_root(), 'data', \"AmazonFineFoodReviews.dataset\")\n",
    "final_datasets = load_from_disk(dataset_path)\n",
    "\n",
    "print(final_datasets)\n",
    "print(f\"\\nTrain: {len(final_datasets['train']):,}개\")\n",
    "print(f\"Validation: {len(final_datasets['validation']):,}개\")\n",
    "print(f\"Test: {len(final_datasets['test']):,}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 셈플10개만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_datasets = DatasetDict({\n",
    "       'train': final_datasets['train'].select(range(10)),\n",
    "       'validation': final_datasets['validation'].select(range(10)),\n",
    "       'test': final_datasets['test'].select(range(10))\n",
    "   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART 토크나이저 로드\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"토크나이제이션 함수\"\"\"\n",
    "    # 입력 (Text)\n",
    "    model_inputs = tokenizer(\n",
    "        examples['Text'],\n",
    "        max_length=200,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # 출력 (Summary)\n",
    "    labels = tokenizer(\n",
    "        examples['Summary'],\n",
    "        max_length=25,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# 토크나이제이션 적용\n",
    "print(\"토크나이제이션 중...\")\n",
    "tokenized_datasets = final_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['Text', 'Summary', 'text_length', 'summary_length'],\n",
    "    desc=\"토크나이제이션\"\n",
    ")\n",
    "\n",
    "print(\"\\n토크나이제이션 완료!\")\n",
    "print(tokenized_datasets)\n",
    "print(f\"\\n샘플 확인:\")\n",
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# 모델 로드\n",
    "print(\"BART 모델 로드 중...\")\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Data Collator (동적 패딩)\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    warmup_steps=500,\n",
    "    fp16=True,  # GPU 있으면 활성화\n",
    "    report_to=\"none\",  # wandb 비활성화\n",
    ")\n",
    "\n",
    "print(\"\\n학습 설정:\")\n",
    "print(f\"  배치 크기: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  에폭: {training_args.num_train_epochs}\")\n",
    "print(f\"  학습률: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "print(\"\\n학습 시작...\")\n",
    "print(\"=\" * 60)\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model_save_path = \"./bart_review_summarizer\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"\\n모델 저장 완료: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "# ROUGE 메트릭 로드\n",
    "rouge_metric = load_metric('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # -100을 패딩 토큰으로 변환\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # 디코딩\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE 계산\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'rouge1': result['rouge1'].mid.fmeasure,\n",
    "        'rouge2': result['rouge2'].mid.fmeasure,\n",
    "        'rougeL': result['rougeL'].mid.fmeasure,\n",
    "    }\n",
    "\n",
    "# 테스트 데이터 평가\n",
    "print(\"\\n테스트 데이터 평가 중...\")\n",
    "test_results = trainer.predict(tokenized_datasets['test'])\n",
    "print(\"\\n평가 결과:\")\n",
    "print(f\"  ROUGE-1: {test_results.metrics.get('test_rouge1', 0):.4f}\")\n",
    "print(f\"  ROUGE-2: {test_results.metrics.get('test_rouge2', 0):.4f}\")\n",
    "print(f\"  ROUGE-L: {test_results.metrics.get('test_rougeL', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, max_length=25):\n",
    "    \"\"\"리뷰 요약 생성\"\"\"\n",
    "    # 토크나이징\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=200,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # GPU 사용 시\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        model.to('cuda')\n",
    "    \n",
    "    # 생성\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "        length_penalty=0.8,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    # 디코딩\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# 테스트\n",
    "test_text = \"\"\"\n",
    "I absolutely love this coffee! The flavor is rich and smooth, \n",
    "not bitter at all. The aroma is amazing and fills the entire kitchen. \n",
    "The packaging keeps it fresh for a long time. \n",
    "Great value for the price. Highly recommend to all coffee lovers!\n",
    "\"\"\"\n",
    "\n",
    "summary = generate_summary(test_text)\n",
    "print(f\"입력: {test_text[:150]}...\")\n",
    "print(f\"\\n생성된 요약: {summary}\")\n",
    "\n",
    "# 여러 샘플 테스트\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"테스트 샘플 3개:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(3):\n",
    "    sample = final_datasets['test'][i]\n",
    "    generated = generate_summary(sample['Text'])\n",
    "    print(f\"\\n[샘플 {i+1}]\")\n",
    "    print(f\"원본: {sample['Text'][:150]}...\")\n",
    "    print(f\"실제 요약: {sample['Summary']}\")\n",
    "    print(f\"생성 요약: {generated}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Text', 'Summary', 'text_length', 'summary_length'],\n",
      "        num_rows: 312979\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Text', 'Summary', 'text_length', 'summary_length'],\n",
      "        num_rows: 39079\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Text', 'Summary', 'text_length', 'summary_length'],\n",
      "        num_rows: 39118\n",
      "    })\n",
      "})\n",
      "\n",
      "Train: 312,979개\n",
      "Validation: 39,079개\n",
      "Test: 39,118개\n",
      "\n",
      "============================================================\n",
      "테스트용 샘플링: 각 10개씩\n",
      "============================================================\n",
      "\n",
      "Train: 10개\n",
      "Validation: 10개\n",
      "Test: 10개\n",
      "\n",
      "샘플 데이터 확인:\n",
      "Text: do not buy this! my cats wouldn't eat it, and within two days, i figured out why. i took it out of t...\n",
      "Summary: worst product ever\n",
      "\n",
      "============================================================\n",
      "토크나이제이션\n",
      "============================================================\n",
      "토크나이제이션 완료!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "\n",
      "============================================================\n",
      "모델 로드\n",
      "============================================================\n",
      "BART 모델 로드 완료!\n",
      "\n",
      "============================================================\n",
      "학습 설정 (테스트용)\n",
      "============================================================\n",
      "학습 설정:\n",
      "  데이터: Train 10개\n",
      "  배치 크기: 2\n",
      "  에폭: 1\n",
      "  학습률: 3e-05\n",
      "\n",
      "============================================================\n",
      "학습 시작\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.803600</td>\n",
       "      <td>10.661615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "학습 완료!\n",
      "\n",
      "============================================================\n",
      "추론 테스트\n",
      "============================================================\n",
      "\n",
      "테스트 샘플 생성:\n",
      "============================================================\n",
      "\n",
      "[샘플 1]\n",
      "입력: green mountain nantucket blend is a palate pleasing mild-medium blend. i would put it more towards t...\n",
      "실제 요약: more mild than medium. smooth. no aftertaste\n",
      "생성 요약: green mountain nantucket blend is a palate pleasing mild-medium blend. i would put it more towards the\n",
      "------------------------------------------------------------\n",
      "\n",
      "[샘플 2]\n",
      "입력: i was a little worried my cat might not like this food after reading some of the reviews. i bought t...\n",
      "실제 요약: my cat loves it\n",
      "생성 요약: i was a little worried my cat might not like this food after reading some of the reviews. i bought the\n",
      "------------------------------------------------------------\n",
      "\n",
      "[샘플 3]\n",
      "입력: hugs, 12-ounce bags (pack of 4) was fresh, very good candy.but i hate to admitt this but they are al...\n",
      "실제 요약: very good\n",
      "생성 요약: hugs, 12-ounce bags (pack of 4) was fresh, very good candy.but i hate\n",
      "------------------------------------------------------------\n",
      "\n",
      "✅ 파이프라인 테스트 완료!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, DatasetDict\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import os\n",
    "\n",
    "# ==================== 1. 데이터 로드 및 샘플링 ====================\n",
    "dataset_path = os.path.join(drive_root(), 'data', \"AmazonFineFoodReviews.dataset\")\n",
    "full_datasets = load_from_disk(dataset_path)\n",
    "\n",
    "print(\"원본 데이터:\")\n",
    "print(full_datasets)\n",
    "print(f\"\\nTrain: {len(full_datasets['train']):,}개\")\n",
    "print(f\"Validation: {len(full_datasets['validation']):,}개\")\n",
    "print(f\"Test: {len(full_datasets['test']):,}개\")\n",
    "\n",
    "# 테스트용 샘플링 (10개씩)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"테스트용 샘플링: 각 10개씩\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    'train': full_datasets['train'].select(range(10)),\n",
    "    'validation': full_datasets['validation'].select(range(10)),\n",
    "    'test': full_datasets['test'].select(range(10))\n",
    "})\n",
    "\n",
    "print(f\"\\nTrain: {len(final_datasets['train'])}개\")\n",
    "print(f\"Validation: {len(final_datasets['validation'])}개\")\n",
    "print(f\"Test: {len(final_datasets['test'])}개\")\n",
    "\n",
    "# 샘플 확인\n",
    "print(\"\\n샘플 데이터 확인:\")\n",
    "sample = final_datasets['train'][0]\n",
    "print(f\"Text: {sample['Text'][:100]}...\")\n",
    "print(f\"Summary: {sample['Summary']}\")\n",
    "\n",
    "# ==================== 2. 토크나이저 및 토크나이제이션 ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토크나이제이션\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"토크나이제이션 함수\"\"\"\n",
    "    # 입력\n",
    "    model_inputs = tokenizer(\n",
    "        examples['Text'],\n",
    "        max_length=200,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # 출력\n",
    "    labels = tokenizer(\n",
    "        examples['Summary'],\n",
    "        max_length=25,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# 토크나이제이션 적용\n",
    "tokenized_datasets = final_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['Text', 'Summary', 'text_length', 'summary_length'],\n",
    "    desc=\"토크나이제이션\"\n",
    ")\n",
    "\n",
    "print(\"토크나이제이션 완료!\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# ==================== 3. 모델 로드 ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"모델 로드\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "print(\"BART 모델 로드 완료!\")\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# ==================== 4. 학습 설정 (테스트용) ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"학습 설정 (테스트용)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_test',\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2,  # 작은 배치\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,  # 1 에폭만\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=2,  # 자주 로깅\n",
    "    warmup_steps=2,\n",
    "    fp16=False,  # CPU 호환\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"학습 설정:\")\n",
    "print(f\"  데이터: Train {len(tokenized_datasets['train'])}개\")\n",
    "print(f\"  배치 크기: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  에폭: {training_args.num_train_epochs}\")\n",
    "print(f\"  학습률: {training_args.learning_rate}\")\n",
    "\n",
    "# ==================== 5. Trainer 및 학습 ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"학습 시작\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 학습\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n학습 완료!\")\n",
    "\n",
    "# ==================== 6. 추론 테스트 ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"추론 테스트\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_summary(text, max_length=25):\n",
    "    \"\"\"요약 생성\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=200,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # GPU 사용 가능 시\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 생성\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "        length_penalty=0.8,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# # 테스트 샘플 3개\n",
    "# print(\"\\n테스트 샘플 생성:\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# for i in range(min(3, len(final_datasets['test']))):\n",
    "#     sample = final_datasets['test'][i]\n",
    "#     generated = generate_summary(sample['Text'])\n",
    "    \n",
    "#     print(f\"\\n[샘플 {i+1}]\")\n",
    "#     print(f\"입력: {sample['Text'][:100]}...\")\n",
    "#     print(f\"실제 요약: {sample['Summary']}\")\n",
    "#     print(f\"생성 요약: {generated}\")\n",
    "#     print(\"-\" * 60)\n",
    "\n",
    "# print(\"\\n✅ 파이프라인 테스트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\sw1\\anaconda3\\envs\\env_colab_250827\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.8.3)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sw1\\anaconda3\\envs\\env_colab_250827\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 9.6 MB/s  0:00:00\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17518 sha256=1ab1f69f490bd60df2ef4d9b0dbff2632b4f41b82609b1197913194054a867ce\n",
      "  Stored in directory: c:\\users\\sw1\\appdata\\local\\pip\\cache\\wheels\\c0\\59\\9f\\7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "\n",
      "   ----------------------------------------  0/11 [rfc3986]\n",
      "  Attempting uninstall: hyperframe\n",
      "   ----------------------------------------  0/11 [rfc3986]\n",
      "    Found existing installation: hyperframe 6.1.0\n",
      "   ----------------------------------------  0/11 [rfc3986]\n",
      "    Uninstalling hyperframe-6.1.0:\n",
      "   ----------------------------------------  0/11 [rfc3986]\n",
      "      Successfully uninstalled hyperframe-6.1.0\n",
      "   ----------------------------------------  0/11 [rfc3986]\n",
      "   --- ------------------------------------  1/11 [hyperframe]\n",
      "  Attempting uninstall: hpack\n",
      "   --- ------------------------------------  1/11 [hyperframe]\n",
      "    Found existing installation: hpack 4.1.0\n",
      "   --- ------------------------------------  1/11 [hyperframe]\n",
      "    Uninstalling hpack-4.1.0:\n",
      "   --- ------------------------------------  1/11 [hyperframe]\n",
      "      Successfully uninstalled hpack-4.1.0\n",
      "   --- ------------------------------------  1/11 [hyperframe]\n",
      "   ------- --------------------------------  2/11 [hpack]\n",
      "   ------- --------------------------------  2/11 [hpack]\n",
      "  Attempting uninstall: h11\n",
      "   ------- --------------------------------  2/11 [hpack]\n",
      "    Found existing installation: h11 0.16.0\n",
      "   ------- --------------------------------  2/11 [hpack]\n",
      "    Uninstalling h11-0.16.0:\n",
      "   ------- --------------------------------  2/11 [hpack]\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "   ------- --------------------------------  2/11 [hpack]\n",
      "   ---------- -----------------------------  3/11 [h11]\n",
      "   ---------- -----------------------------  3/11 [h11]\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "  Attempting uninstall: idna\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "    Found existing installation: idna 3.10\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "    Uninstalling idna-3.10:\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "      Successfully uninstalled idna-3.10\n",
      "   -------------- -------------------------  4/11 [chardet]\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "  Attempting uninstall: h2\n",
      "   ------------------ ---------------------  5/11 [idna]\n",
      "   ------------------------- --------------  7/11 [h2]\n",
      "    Found existing installation: h2 4.3.0\n",
      "   ------------------------- --------------  7/11 [h2]\n",
      "    Uninstalling h2-4.3.0:\n",
      "   ------------------------- --------------  7/11 [h2]\n",
      "      Successfully uninstalled h2-4.3.0\n",
      "   ------------------------- --------------  7/11 [h2]\n",
      "   ------------------------- --------------  7/11 [h2]\n",
      "  Attempting uninstall: httpcore\n",
      "   ------------------------- --------------  7/11 [h2]\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "   ------------------------- --------------  7/11 [h2]\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "   ------------------------- --------------  7/11 [h2]\n",
      "   ----------------------------- ----------  8/11 [httpcore]\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "   ----------------------------- ----------  8/11 [httpcore]\n",
      "   ----------------------------- ----------  8/11 [httpcore]\n",
      "   ----------------------------- ----------  8/11 [httpcore]\n",
      "  Attempting uninstall: httpx\n",
      "   ----------------------------- ----------  8/11 [httpcore]\n",
      "    Found existing installation: httpx 0.28.1\n",
      "   ----------------------------- ----------  8/11 [httpcore]\n",
      "    Uninstalling httpx-0.28.1:\n",
      "   ----------------------------- ----------  8/11 [httpcore]\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "   ----------------------------- ----------  8/11 [httpcore]\n",
      "   -------------------------------- -------  9/11 [httpx]\n",
      "   -------------------------------- -------  9/11 [httpx]\n",
      "   ------------------------------------ --- 10/11 [googletrans]\n",
      "   ---------------------------------------- 11/11 [googletrans]\n",
      "\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'googletrans' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'googletrans'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-genai 1.31.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
      "gradio 5.43.1 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
      "gradio-client 1.12.1 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
      "mcp 1.13.1 requires httpx>=0.27.1, but you have httpx 0.13.3 which is incompatible.\n",
      "openai 1.101.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "roboflow 1.2.7 requires idna==3.7, but you have idna 2.10 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip -q install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 샘플 생성:\n",
      "============================================================\n",
      "\n",
      "[샘플 1]\n",
      "------------------------------------------------------------\n",
      "📝 입력 (영문):\n",
      "  green mountain nantucket blend is a palate pleasing mild-medium blend. i would put it more towards the mild side than the medium side. it is very smoo...\n",
      "\n",
      "✅ 실제 요약 (영문): more mild than medium. smooth. no aftertaste\n",
      "🤖 생성 요약 (영문): green mountain nantucket blend is a palate pleasing mild-medium blend. i would put it more towards the\n",
      "\n",
      "============================================================\n",
      "🇰🇷 한글 번역:\n",
      "============================================================\n",
      "📝 입력 (한글):\n",
      "  Green Mountain Nantucket Blend는 구개를 유쾌한 온화한 중간 조합입니다.나는 그것을 중간면보다 온화한쪽으로 더 넣을 것입니다.매우 스모입니다...\n",
      "\n",
      "✅ 실제 요약 (한글): 중간보다 온화합니다.매끄러운.뒷맛이 없습니다\n",
      "🤖 생성 요약 (한글): Green Mountain Nantucket Blend는 구개를 유쾌한 온화한 중간 조합입니다.나는 그것을 더 많이 넣을 것이다\n",
      "------------------------------------------------------------\n",
      "\n",
      "[샘플 2]\n",
      "------------------------------------------------------------\n",
      "📝 입력 (영문):\n",
      "  i was a little worried my cat might not like this food after reading some of the reviews. i bought the turkey and salmon formula and this is the best ...\n",
      "\n",
      "✅ 실제 요약 (영문): my cat loves it\n",
      "🤖 생성 요약 (영문): i was a little worried my cat might not like this food after reading some of the reviews. i bought the\n",
      "\n",
      "============================================================\n",
      "🇰🇷 한글 번역:\n",
      "============================================================\n",
      "📝 입력 (한글):\n",
      "  나는 내 고양이가 리뷰 중 일부를 읽은 후이 음식을 좋아하지 않을까 걱정했다.나는 칠면조와 연어 공식을 샀는데 이것이 최고이다...\n",
      "\n",
      "✅ 실제 요약 (한글): 내 고양이는 그것을 좋아합니다\n",
      "🤖 생성 요약 (한글): 나는 내 고양이가 리뷰 중 일부를 읽은 후이 음식을 좋아하지 않을까 걱정했다.나는 샀다\n",
      "------------------------------------------------------------\n",
      "\n",
      "[샘플 3]\n",
      "------------------------------------------------------------\n",
      "📝 입력 (영문):\n",
      "  hugs, 12-ounce bags (pack of 4) was fresh, very good candy.but i hate to admitt this but they are all gone...\n",
      "\n",
      "✅ 실제 요약 (영문): very good\n",
      "🤖 생성 요약 (영문): hugs, 12-ounce bags (pack of 4) was fresh, very good candy.but i hate\n",
      "\n",
      "============================================================\n",
      "🇰🇷 한글 번역:\n",
      "============================================================\n",
      "📝 입력 (한글):\n",
      "  포옹, 12 온스 백 (4 팩)은 신선하고 아주 좋은 사탕 이었지만 나는 이것을 인정하는 것을 싫어하지만 모두 사라졌습니다....\n",
      "\n",
      "✅ 실제 요약 (한글): 매우 좋은\n",
      "🤖 생성 요약 (한글): 포옹, 12 온스 가방 (4 팩)은 신선하고 아주 좋은 사탕 이었지만 나는 싫어합니다.\n",
      "------------------------------------------------------------\n",
      "\n",
      "파이프라인 테스트 완료!\n"
     ]
    }
   ],
   "source": [
    "# 설치 (처음 한 번만)\n",
    "# !pip install googletrans==4.0.0-rc1\n",
    "\n",
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "# 번역기 초기화\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_korean(text):\n",
    "    \"\"\"영문을 한글로 번역\"\"\"\n",
    "    try:\n",
    "        result = translator.translate(text, src='en', dest='ko')\n",
    "        time.sleep(0.5)  # API 제한 방지\n",
    "        return result.text\n",
    "    except Exception as e:\n",
    "        return f\"[번역 실패: {str(e)}]\"\n",
    "\n",
    "# ==================== 테스트 샘플 생성 (번역 추가) ====================\n",
    "print(\"\\n테스트 샘플 생성:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(3, len(final_datasets['test']))):\n",
    "    sample = final_datasets['test'][i]\n",
    "    generated = generate_summary(sample['Text'])\n",
    "    \n",
    "    print(f\"\\n[샘플 {i+1}]\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 영문 출력\n",
    "    print(\"📝 입력 (영문):\")\n",
    "    print(f\"  {sample['Text'][:150]}...\")\n",
    "    print(f\"\\n✅ 실제 요약 (영문): {sample['Summary']}\")\n",
    "    print(f\"🤖 생성 요약 (영문): {generated}\")\n",
    "    \n",
    "    # 한글 번역\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🇰🇷 한글 번역:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    input_ko = translate_to_korean(sample['Text'][:150])\n",
    "    print(f\"📝 입력 (한글):\")\n",
    "    print(f\"  {input_ko}...\")\n",
    "    \n",
    "    actual_ko = translate_to_korean(sample['Summary'])\n",
    "    print(f\"\\n✅ 실제 요약 (한글): {actual_ko}\")\n",
    "    \n",
    "    generated_ko = translate_to_korean(generated)\n",
    "    print(f\"🤖 생성 요약 (한글): {generated_ko}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n파이프라인 테스트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "name": "",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "env_colab_250827",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
