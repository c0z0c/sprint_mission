---
layout: default
title: "위클리페이퍼4 4팀_김명환 - 머신러닝 vs 딥러닝 그리고 하이퍼파라미터"
description: "머신러닝 vs 딥러닝 그리고 하이퍼파라미터"
date: 2025-07-31
cache-control: no-cache
expires: 0
pragma: no-cache
---

# 위클리페이퍼 #4: 머신러닝 vs 딥러닝 그리고 하이퍼파라미터

---

## 문서 정보

| 항목 | 내용 |
|------|------|
| **작성자** | 김명환 |
| **작성일** | 2025년 7월 31일 |
| **과정** | AI엔지니어 딥러닝 과정 |
| **주제** | 머신러닝과 딥러닝의 차이점 및 하이퍼파라미터 최적화 |
---

## 📌 학습 목표

이 문서를 통해 다음 내용을 학습할 수 있습니다:

- ✅ 머신러닝과 딥러닝의 포함관계와 핵심 차이점 이해
- ✅ 딥러닝 성능 향상을 위한 주요 하이퍼파라미터 숙지
- ✅ 실무 환경에서의 메모리 관리와 최적 설정 방법

---

---

## 단원 1: 딥러닝과 머신러닝의 포함관계

### 1.1 기본 개념 이해하기

머신러닝(Machine Learning)과 딥러닝(Deep Learning)의 관계를 이해하기 위해서는 먼저 인공지능(AI)의 전체적인 구조를 알아야 합니다.

```
인공지능(AI)
    └── 머신러닝(Machine Learning)
            └── 딥러닝(Deep Learning)
```

### 1.2 각 개념의 정의

#### 인공지능(Artificial Intelligence)
- **정의**: 인간의 지능을 모방하여 문제를 해결하는 모든 기술
- **예시**: 규칙 기반 시스템, Expert System(익스퍼트 시스템), 머신러닝 등

#### 머신러닝(Machine Learning)
- **정의**: 명시적으로 Programming(프로그래밍)하지 않고도 Data(데이터)로부터 학습하여 성능을 개선하는 Algorithm(알고리즘)
- **특징**: 
  - 데이터에서 Pattern(패턴)을 자동으로 학습
  - 새로운 데이터에 대한 Prediction(프리딕션) 가능
  - 전통적인 알고리즘: Linear Regression(리니어 리그레션), Decision Tree(디시전 트리), SVM(서포트 벡터 머신), Random Forest(랜덤 포레스트) 등

#### 딥러닝(Deep Learning)
- **정의**: Artificial Neural Network(아티피셜 뉴럴 네트워크)를 여러 층으로 깊게 쌓아 복잡한 패턴을 학습하는 머신러닝의 한 분야
- **특징**:
  - 3개 이상의 Hidden Layer(히든 레이어)를 가진 Neural Network(뉴럴 네트워크)
  - 자동으로 Feature(피처)를 Extract(익스트랙트)
  - 대량의 데이터와 Computing Power(컴퓨팅 파워)가 필요

### 1.3 머신러닝과 딥러닝의 주요 차이점

| 구분 | 머신러닝 | 딥러닝 |
|------|----------|---------|
| **특징 추출** | 수동으로 특징 설계 필요 | 자동으로 특징 학습 |
| **데이터 양** | 적은 데이터로도 가능 | 대량의 데이터 필요 |
| **계산 복잡도** | 상대적으로 낮음 | 매우 높음 (GPU 필요) |
| **해석 가능성** | 비교적 해석 가능 | Black Box(블랙 박스) 성격이 강함 |
| **학습 시간** | 상대적으로 짧음 | 매우 오래 걸림 |

### 1.4 실제 예시로 이해하기

**Image Classification(이미지 클래시피케이션) 문제**를 예로 들어보겠습니다:

- **전통적인 머신러닝 접근법**:
  1. 이미지에서 특징 추출 (예: Edge(엣지), Corner(코너), Color Histogram(컬러 히스토그램))
  2. 추출한 특징을 SVM이나 Random Forest(랜덤 포레스트)에 Input(인풋)
  3. 분류 결과 Output(아웃풋)

- **딥러닝 접근법**:
  1. 원본 이미지를 그대로 CNN(Convolutional Neural Network, 컨볼루셔널 뉴럴 네트워크)에 입력
  2. CNN이 자동으로 특징을 학습하고 분류
  3. 더 높은 Accuracy(어큐러시) 달성

---

## 단원 2: 딥러닝 성능향상을 위한 주요 하이퍼파라미터

하이퍼파라미터(Hyperparameter)는 Model(모델) 학습 전에 사용자가 설정해야 하는 값들입니다. 올바른 하이퍼파라미터 설정은 모델 Performance(퍼포먼스)에 큰 영향을 미칩니다.

### 2.1 Learning Rate(러닝 레이트)

**정의**: Weight(웨이트)를 Update(업데이트)하는 속도를 결정하는 Parameter(파라미터)

```python
# 예시
optimizer = Adam(learning_rate=0.001)  # 기본값
```

**특징**:
- **너무 큰 경우**: Optimal Point(옵티멀 포인트)를 넘어서 Divergence(다이버전스)할 수 있음
- **너무 작은 경우**: 학습이 매우 느리고 Local Minimum(로컬 미니멈)에 갇힐 수 있음
- **일반적인 값**: 0.001, 0.01, 0.1
- **적응적 방법**: Learning Rate Scheduling(러닝 레이트 스케줄링) 사용

### 2.2 Batch Size(배치 사이즈)

**정의**: 한 번의 가중치 업데이트에 사용되는 데이터 Sample(샘플)의 수

```python
model.fit(X_train, y_train, batch_size=32)  # 일반적인 값
```

**특징**:
- **큰 배치 크기**: 
  - 장점: 안정적인 학습, GPU 효율성 ↑
  - 단점: Memory 사용량 ↑, Generalization(제너럴라이제이션) 성능 ↓
- **작은 배치 크기**:
  - 장점: 더 나은 일반화, 메모리 효율적
  - 단점: 학습 불안정, 시간 오래 걸림
- **일반적인 값**: 16, 32, 64, 128, 256

### 2.3 Epochs(에포크) 수

**정의**: 전체 Dataset을 몇 번 반복해서 학습할지 결정

```python
history = model.fit(X_train, y_train, epochs=100)
```

**특징**:
- **너무 적으면**: Underfitting(언더피팅) - 학습 부족
- **너무 많으면**: Overfitting(오버피팅) - 과적합
- **해결 방법**: Early Stopping(얼리 스토핑) 사용
- **일반적인 범위**: 10 ~ 1000 (데이터와 모델에 따라 다름)

### 2.4 은닉층 수와 뉴런 수 (Network Architecture, 네트워크 아키텍처)

**정의**: 신경망의 Depth(뎁스)와 Width(위드스)를 결정

```python
model = Sequential([
    Dense(128, activation='relu'),  # 첫 번째 은닉층: 128개 뉴런
    Dense(64, activation='relu'),   # 두 번째 은닉층: 64개 뉴런
    Dense(32, activation='relu'),   # 세 번째 은닉층: 32개 뉴런
    Dense(10, activation='softmax') # 출력층
])
```

**설계 원칙**:
- **깊은 네트워크**: 복잡한 패턴 학습 가능, 과적합 위험 ↑
- **얕은 네트워크**: 단순한 패턴만 학습, 학습 속도 빠름
- **뉴런 수**: 보통 2의 거듭제곱 사용 (32, 64, 128, 256...)

### 2.5 Dropout Rate(드롭아웃 레이트)

**정의**: 학습 중 Random하게 Neuron을 Deactivate(디액티베이트)하는 비율

```python
model.add(Dropout(0.5))  # 50%의 뉴런을 랜덤하게 끔
```

**특징**:
- **목적**: 과적합 방지
- **일반적인 값**: 0.2 ~ 0.5
- **너무 높으면**: Underfitting(언더피팅) 발생
- **적용 위치**: 주로 Fully Connected Layer(풀리 커넥티드 레이어) 사이

### 2.6 활성화 함수(Activation Function)

**정의**: 뉴런의 출력을 결정하는 Non-linear Function(논리니어 펑션)

```python
# 은닉층
Dense(64, activation='relu')      # ReLU(렐루) 함수
Dense(64, activation='tanh')      # Tanh(탄에이치) 함수
Dense(64, activation='sigmoid')   # Sigmoid(시그모이드) 함수

# 출력층
Dense(1, activation='sigmoid')    # Binary Classification(바이너리 클래시피케이션)
Dense(10, activation='softmax')   # Multi-class Classification(멀티클래스 클래시피케이션)
```

**선택 가이드**:
- **은닉층**: ReLU(렐루)가 일반적 (빠르고 효과적)
- **출력층**: 
  - 이진 분류: Sigmoid(시그모이드)
  - 다중 분류: Softmax(소프트맥스)
  - Regression(리그레션): Linear(리니어) - 활성화 함수 없음

### 2.7 Optimizer(옵티마이저)

**정의**: 가중치를 업데이트하는 알고리즘

```python
# 다양한 옵티마이저
model.compile(optimizer='sgd')      # SGD(에스지디) - Stochastic Gradient Descent(스토캐스틱 그래디언트 디센트)
model.compile(optimizer='adam')     # Adam(아담) - Adaptive Moment Estimation(어댑티브 모먼트 에스티메이션)
model.compile(optimizer='rmsprop')  # RMSprop(알엠에스프롭) - Root Mean Square Propagation(루트 민 스퀘어 프로퍼게이션)
model.compile(optimizer='adagrad')  # Adagrad(아다그래드) - Adaptive Gradient(어댑티브 그래디언트)
```

**선택 가이드**:
- **Adam(아담)**: 대부분의 경우 좋은 성능 (기본 선택)
- **SGD(에스지디)**: 학습률 조정 필요, 때로는 더 나은 일반화
- **RMSprop(알엠에스프롭)**: RNN(Recurrent Neural Network, 리커런트 뉴럴 네트워크)에서 자주 사용

### 2.8 정규화(Regularization) 파라미터

**정의**: 과적합을 방지하기 위한 Penalty(페널티) 항

#### 2.8.1 정규화의 기본 개념

정규화는 모델이 훈련 데이터에 과도하게 맞춰지는 것을 방지하기 위해 손실함수에 추가적인 항을 더하는 기법입니다. 마치 "너무 복잡하게 생각하지 말고 단순하게 가자!"라고 모델에게 말해주는 것과 같습니다.

**기본 손실함수 구조:**
```
전체 손실 = 원래 손실 + λ × 정규화 항
```

여기서 λ(람다)는 정규화의 강도를 조절하는 하이퍼파라미터입니다.

#### 2.8.2 L1 정규화 (Lasso Regularization)

**수학적 정의:**
```
L1 정규화 항 = λ × Σ|w_i|
```

- w_i: 각 가중치
- |w_i|: 가중치의 절댓값
- Σ: 모든 가중치에 대한 합

**특징:**
- **Sparsity(희소성) 유도**: 일부 가중치를 정확히 0으로 만듦
- **Feature Selection(특징 선택) 효과**: 중요하지 않은 특징을 자동으로 제거
- **해석 가능성**: 0이 된 가중치는 해당 특징이 불필요함을 의미

**시각적 이해:**
L1은 가중치에 "절댓값"을 적용하기 때문에, 다이아몬드 모양의 제약 조건을 만듭니다.

```python
# L1 정규화 예시
from tensorflow.keras.regularizers import l1

model.add(Dense(64, kernel_regularizer=l1(0.01)))
```

**L1의 장점:**
- 자동으로 중요한 특징만 선택
- 모델이 단순해짐
- 메모리 효율적

**단점:**
- 상관관계가 높은 특징들 중 하나만 선택할 수 있음
- 미분이 불가능한 점(0에서) 존재

#### 2.8.3 L2 정규화 (Ridge Regularization)

**수학적 정의:**
```
L2 정규화 항 = λ × Σ(w_i)²
```

- w_i: 각 가중치
- (w_i)²: 가중치의 제곱
- Σ: 모든 가중치에 대한 합

**특징:**
- **가중치 감소**: 가중치를 0에 가깝게 만들지만 정확히 0으로 만들지는 않음
- **부드러운 제약**: 모든 특징을 조금씩 사용
- **안정성**: 수치적으로 안정적

**시각적 이해:**
L2는 가중치에 "제곱"을 적용하기 때문에, 원형의 제약 조건을 만듭니다.

```python
# L2 정규화 예시
from tensorflow.keras.regularizers import l2

model.add(Dense(64, kernel_regularizer=l2(0.01)))
```

**L2의 장점:**
- 모든 특징을 균형있게 사용
- 미분 가능하여 최적화가 쉬움
- 가중치가 너무 커지는 것을 방지

**단점:**
- 완전한 특징 선택은 불가능
- 해석이 상대적으로 어려움

#### 2.8.4 L1 vs L2 비교

| 구분 | L1 정규화 | L2 정규화 |
|------|----------|----------|
| **수학식** | λ × Σ\|w_i\| | λ × Σ(w_i)² |
| **기하학적 형태** | 다이아몬드 | 원 |
| **가중치 효과** | 일부를 정확히 0으로 | 모든 것을 작게 |
| **특징 선택** | 자동 선택 | 균등하게 감소 |
| **해석성** | 높음 | 보통 |
| **수치 안정성** | 보통 | 높음 |

#### 2.8.5 Elastic Net (L1 + L2 조합)

**수학적 정의:**
```
Elastic Net = λ₁ × Σ|w_i| + λ₂ × Σ(w_i)²
```

- λ₁: L1 정규화 강도
- λ₂: L2 정규화 강도

```python
# L1 + L2 정규화 조합
from tensorflow.keras.regularizers import l1_l2

model.add(Dense(64, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))
```

**장점:**
- L1의 특징 선택 능력 + L2의 안정성
- 상관관계가 높은 특징들을 그룹으로 선택 가능

#### 2.8.6 실제 적용 예시

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.regularizers import l1, l2, l1_l2

# 정규화 없는 모델
model_none = Sequential([
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# L1 정규화 모델
model_l1 = Sequential([
    Dense(128, activation='relu', kernel_regularizer=l1(0.01)),
    Dense(64, activation='relu', kernel_regularizer=l1(0.01)),
    Dense(10, activation='softmax')
])

# L2 정규화 모델
model_l2 = Sequential([
    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    Dense(10, activation='softmax')
])

# Elastic Net 모델
model_elastic = Sequential([
    Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
    Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
    Dense(10, activation='softmax')
])
```

#### 2.8.7 정규화 강도(λ) 선택 가이드

**일반적인 값 범위:**
- **시작값**: 0.001 ~ 0.01
- **강한 정규화**: 0.1 ~ 1.0
- **약한 정규화**: 0.0001 ~ 0.001

**선택 기준:**
1. **과적합이 심한 경우**: λ 값을 크게 (0.1 이상)
2. **모델이 단순한 경우**: λ 값을 작게 (0.001 이하)
3. **특징이 많은 경우**: L1 선택
4. **특징들이 상관관계가 높은 경우**: L2 또는 Elastic Net 선택

**튜닝 방법:**
```python
# 교차 검증을 통한 λ 값 선택
lambda_values = [0.001, 0.01, 0.1, 1.0]
best_lambda = None
best_score = 0

for lam in lambda_values:
    model = create_model_with_l2(lam)
    score = cross_validate(model, X_train, y_train)
    if score > best_score:
        best_score = score
        best_lambda = lam

print(f"최적 λ 값: {best_lambda}")
```

#### 2.8.8 실전 팁

**언제 L1을 사용할까?**
- 특징이 매우 많을 때 (예: 텍스트 데이터, 유전자 데이터)
- 해석 가능한 모델이 필요할 때
- 메모리나 계산 자원이 제한적일 때

**언제 L2를 사용할까?**
- 모든 특징이 어느 정도 중요할 것으로 예상될 때
- 안정적인 학습이 중요할 때
- 상관관계가 높은 특징들이 많을 때

**언제 Elastic Net을 사용할까?**
- L1과 L2의 장점을 모두 원할 때
- 특징들 간의 그룹 효과가 있을 때
- 더 유연한 정규화가 필요할 때








## 3. 하이퍼파라미터 튜닝 전략

### 3.1 Grid Search(그리드 서치)
모든 조합을 시도하는 방법
```python
param_grid = {
    'learning_rate': [0.001, 0.01, 0.1],
    'batch_size': [16, 32, 64],
    'dropout_rate': [0.2, 0.3, 0.5]
}
```

### 3.2 Random Search(랜덤 서치)
랜덤하게 조합을 선택하는 방법
- Grid Search보다 효율적
- 더 넓은 범위 탐색 가능

### 3.3 Bayesian Optimization(베이지안 옵티마이제이션)
이전 결과를 바탕으로 다음 시도를 결정
- 가장 효율적
- 구현이 복잡함

### 3.4 실용적인 접근법

1. **기본값으로 시작**
   - Learning Rate: 0.001
   - Batch Size: 32
   - Optimizer: Adam(아담)
   - Activation: ReLU(렐루) - 은닉층

2. **한 번에 하나씩 조정**
   - Learning Rate를 먼저 조정
   - 그 다음 Batch Size
   - 네트워크 구조는 마지막에

3. **검증 세트 사용**
   - 항상 별도의 Validation Set(밸리데이션 셋)로 Evaluation(이밸류에이션)
   - 과적합 여부 확인

4. **Early Stopping(얼리 스토핑) 활용**
   ```python
   early_stop = EarlyStopping(monitor='val_loss', patience=10)
   model.fit(X_train, y_train, callbacks=[early_stop])
   ```

## 4. 실전 팁

### 4.1 시작할 때
- 작은 모델로 시작해서 점진적으로 복잡도 증가
- 데이터를 잘 이해하고 Preprocessing(프리프로세싱)에 신경 쓰기
- 기본 하이퍼파라미터로 Baseline(베이스라인) 설정

### 4.2 문제 해결
- **학습이 안 될 때**: Learning Rate 낮추기
- **과적합일 때**: Dropout 추가, L2 정규화, Data Augmentation(데이터 어그멘테이션)
- **학습이 느릴 때**: Batch Size 증가, Learning Rate 증가

### 4.3 Monitoring(모니터링)
- TensorBoard(텐서보드) 사용하여 학습 과정 Visualization(비주얼라이제이션)
- Loss(로스)와 Accuracy(어큐러시)를 함께 관찰
- Training(트레이닝)과 Validation(밸리데이션) 성능 차이 주목

---

## 부록: 전문 용어 해설

### A. 주요 개념 용어

#### Local Minimum(로컬 미니멈)
전체 최솟값이 아닌 특정 영역에서만 최솟값인 지점입니다. 딥러닝에서는 이 지점에 갇히면 더 좋은 해를 찾지 못할 수 있어 문제가 됩니다. Learning Rate가 너무 작으면 Local Minimum에서 벗어나기 어려워집니다. Adam과 같은 적응적 옵티마이저가 이 문제를 완화하는 데 도움이 됩니다.

#### Overfitting(오버피팅)
모델이 훈련 데이터에만 과도하게 맞춰져서 새로운 데이터에 대한 예측 성능이 떨어지는 현상입니다. 훈련 정확도는 높지만 검증 정확도가 낮을 때 발생합니다. Dropout, L2 정규화, Early Stopping 등으로 방지할 수 있습니다.

#### Underfitting(언더피팅)
모델이 너무 단순해서 데이터의 패턴을 제대로 학습하지 못하는 현상입니다. 훈련 데이터와 검증 데이터 모두에서 성능이 낮게 나타납니다. 모델 복잡도를 높이거나 학습 시간을 늘려서 해결할 수 있습니다.

#### Gradient Descent(그래디언트 디센트)
손실함수의 기울기(gradient)를 계산하여 가중치를 업데이트하는 최적화 알고리즘입니다. 산을 내려가듯이 기울기가 가장 가파른 방향으로 이동하여 최솟값을 찾습니다. SGD, Adam 등은 모두 이 원리를 기반으로 합니다.

#### Feature(피처)
모델이 학습하는 데 사용하는 입력 데이터의 특성이나 속성입니다. 전통적인 머신러닝에서는 사람이 직접 설계해야 하지만, 딥러닝에서는 모델이 자동으로 유용한 피처를 추출합니다. 좋은 피처 선택이 모델 성능에 큰 영향을 미칩니다.

#### Backpropagation(백프로퍼게이션)
신경망에서 출력층부터 입력층까지 거꾸로 가면서 각 가중치의 기울기를 계산하는 알고리즘입니다. 체인 룰(Chain Rule)을 사용하여 복합함수의 미분을 효율적으로 계산합니다. 딥러닝의 핵심 학습 메커니즘입니다.

### B. 네트워크 구조 용어

#### CNN(Convolutional Neural Network)
이미지 처리에 특화된 신경망으로, 합성곱(convolution) 연산을 사용합니다. 이미지의 공간적 특성을 보존하면서 특징을 추출할 수 있어 이미지 분류, 객체 탐지 등에 효과적입니다. 파라미터 수가 적어 효율적이고 위치 불변성을 가집니다.

#### RNN(Recurrent Neural Network)
순차적 데이터(시계열, 텍스트)를 처리하는 신경망입니다. 이전 시점의 정보를 기억할 수 있는 메모리 기능을 가지고 있어 문장 생성, 번역 등에 사용됩니다. 하지만 긴 시퀀스에서는 기울기 소실 문제가 발생할 수 있습니다.

#### Dense Layer(덴스 레이어)
모든 뉴런이 이전 층의 모든 뉴런과 연결된 완전연결층입니다. Fully Connected Layer라고도 불리며, 추출된 특징들을 조합하여 최종 예측을 수행합니다. 일반적으로 네트워크의 마지막 부분에 위치합니다.

### C. 성능 지표 용어

#### Loss Function(로스 펑션)
모델의 예측값과 실제값 사이의 차이를 측정하는 함수입니다. 분류 문제에서는 Cross-entropy, 회귀 문제에서는 Mean Squared Error를 주로 사용합니다. 이 값을 최소화하는 것이 학습의 목표입니다.

#### Accuracy(어큐러시)
전체 예측 중 올바르게 예측한 비율을 나타내는 지표입니다. 분류 문제에서 가장 직관적인 성능 지표이지만, 데이터가 불균형할 때는 정확한 평가가 어려울 수 있습니다. 이런 경우 Precision, Recall 등을 함께 고려해야 합니다.

#### Validation Set(밸리데이션 셋)
모델의 성능을 객관적으로 평가하기 위해 훈련에 사용하지 않고 따로 분리한 데이터셋입니다. 하이퍼파라미터 튜닝과 모델 선택에 사용되며, 과적합을 조기에 발견할 수 있게 해줍니다.

---

## 부록: 환경별 설정 가이드

### A. 메모리별 배치 사이즈 권장 설정

#### 16GB RAM 노트북 (GPU 없음)
- **권장 Batch Size**: 8-16
- **최대 모델 크기**: 간단한 MLP, 작은 CNN
- **주의사항**: 큰 이미지나 복잡한 모델은 피하고, 데이터 전처리 시 메모리 관리 중요

```python
# 16GB 환경 설정 예시
model.fit(X_train, y_train, 
          batch_size=8,
          validation_split=0.2,
          epochs=50)
```

#### 32GB RAM 노트북 (GPU 없음)
- **권장 Batch Size**: 16-32
- **최대 모델 크기**: 중간 규모 CNN, 작은 Transformer
- **활용 팁**: 더 큰 배치 사이즈로 안정적인 학습 가능

```python
# 32GB 환경 설정 예시
model.fit(X_train, y_train, 
          batch_size=16,
          validation_split=0.2,
          epochs=100)
```

### B. GPU 메모리별 설정 (추가 GPU가 있는 경우)

#### 4GB GPU (GTX 1650, RTX 3050 등)
- **권장 Batch Size**: 16-32
- **이미지 크기**: 224x224 이하
- **모델**: ResNet18, MobileNet 수준

#### 8GB GPU (RTX 3070, RTX 4060 등)
- **권장 Batch Size**: 32-64
- **이미지 크기**: 512x512까지 가능
- **모델**: ResNet50, EfficientNet 수준

#### 12GB+ GPU (RTX 3080Ti, RTX 4070Ti 이상)
- **권장 Batch Size**: 64-128
- **이미지 크기**: 1024x1024까지 가능
- **모델**: ResNet101, ViT 등 대형 모델 가능

### C. 메모리 부족 시 해결 방법

#### 1. 배치 사이즈 줄이기
```python
# 메모리 부족 시
model.fit(X_train, y_train, 
          batch_size=4,  # 더 작게 설정
          epochs=100)
```

#### 2. 이미지 크기 줄이기
```python
# 이미지 리사이징
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rescale=1./255,
    preprocessing_function=lambda x: tf.image.resize(x, [128, 128])  # 작게 리사이즈
)
```

#### 3. 모델 경량화
```python
# 더 간단한 모델 사용
model = Sequential([
    Conv2D(32, 3, activation='relu'),  # 필터 수 줄이기
    MaxPooling2D(),
    Conv2D(64, 3, activation='relu'),
    MaxPooling2D(),
    Flatten(),
    Dense(128, activation='relu'),     # 뉴런 수 줄이기
    Dense(10, activation='softmax')
])
```

#### 4. Gradient Accumulation(그래디언트 어큐뮬레이션)
```python
# 작은 배치를 여러 번 누적하여 큰 배치 효과
accumulate_grad_batches = 4
effective_batch_size = batch_size * accumulate_grad_batches  # 실질적으로 큰 배치 사이즈 효과
```

### D. 실무 팁

#### 메모리 모니터링
```python
# 메모리 사용량 확인
import psutil
print(f"RAM 사용률: {psutil.virtual_memory().percent}%")

# GPU 메모리 확인 (GPU 있는 경우)
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    tf.config.experimental.set_memory_growth(gpus[0], True)  # 메모리 점진적 증가
```

#### 데이터 로딩 최적화
```python
# 메모리 효율적인 데이터 로딩
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
```

## 마무리

딥러닝은 머신러닝의 한 분야로, 더 복잡한 문제를 해결할 수 있지만 그만큼 더 많은 데이터와 계산 자원이 필요합니다. 하이퍼파라미터 튜닝은 딥러닝 모델의 성능을 크게 좌우하므로, 각 파라미터의 역할을 이해하고 체계적으로 접근하는 것이 중요합니다.

처음에는 기본값으로 시작하여 점진적으로 조정해 나가며, 항상 검증 데이터로 성능을 확인하는 것을 잊지 마세요! 특히 제한된 하드웨어 환경에서는 메모리 관리와 배치 사이즈 설정이 성공적인 학습의 핵심입니다.